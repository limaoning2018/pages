<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>CSUR4503-34</title><meta name="author" content="John David N. Dionisio, William G. Burns III, Richard Gilbert"/><meta name="keywords" content="Immersion, Internet, second life, virtual worlds"/><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 h1 { color: black; font-family:Helvetica, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s1 { color: black; font-family:Helvetica, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s3 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .a, a { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 h2 { color: black; font-family:Helvetica, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; margin:0pt; }
 .s4 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s5 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s6 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s7 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s8 { color: #FFF; font-family:Helvetica, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s9 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s10 { color: black; font-family:Helvetica, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s11 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s12 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 .s13 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s14 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s15 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 3pt; }
 .s16 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt; }
 .s17 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s18 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 .s19 { color: black; font-family:Helvetica, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s20 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 h4 { color: #231F20; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 5.5pt; }
 .s21 { color: #231F20; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 .s23 { color: #231F20; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4pt; }
 .s24 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6.5pt; vertical-align: 3pt; }
 .s25 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5.5pt; }
 .s27 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 1; }
 #l1> li>*:first-child:before {counter-increment: c1; content: counter(c1, decimal)". "; color: black; font-family:Helvetica, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 #l1> li:first-child>*:first-child:before {counter-increment: c1 0;  }
 #l2 {padding-left: 0pt;counter-reset: c2 1; }
 #l2> li>*:first-child:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)". "; color: black; font-family:Helvetica, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 #l2> li:first-child>*:first-child:before {counter-increment: c2 0;  }
 #l3 {padding-left: 0pt;counter-reset: d1 1; }
 #l3> li>*:first-child:before {counter-increment: d1; content: "("counter(d1, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l3> li:first-child>*:first-child:before {counter-increment: d1 0;  }
 #l4 {padding-left: 0pt;counter-reset: c2 1; }
 #l4> li>*:first-child:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)". "; color: black; font-family:Helvetica, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 #l4> li:first-child>*:first-child:before {counter-increment: c2 0;  }
 #l5 {padding-left: 0pt;counter-reset: e1 1; }
 #l5> li>*:first-child:before {counter-increment: e1; content: "("counter(e1, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l5> li:first-child>*:first-child:before {counter-increment: e1 0;  }
 #l6 {padding-left: 0pt;counter-reset: e2 1; }
 #l6> li>*:first-child:before {counter-increment: e2; content: "("counter(e2, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l6> li:first-child>*:first-child:before {counter-increment: e2 0;  }
 #l7 {padding-left: 0pt;counter-reset: c2 1; }
 #l7> li>*:first-child:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)". "; color: black; font-family:Helvetica, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 9pt; }
 #l7> li:first-child>*:first-child:before {counter-increment: c2 0;  }
 #l8 {padding-left: 0pt;counter-reset: c3 1; }
 #l8> li>*:first-child:before {counter-increment: c3; content: counter(c1, decimal)"."counter(c2, decimal)"."counter(c3, decimal)". "; color: black; font-family:Helvetica, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l8> li:first-child>*:first-child:before {counter-increment: c3 0;  }
 #l9 {padding-left: 0pt;counter-reset: f1 1; }
 #l9> li>*:first-child:before {counter-increment: f1; content: "("counter(f1, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l9> li:first-child>*:first-child:before {counter-increment: f1 0;  }
 #l10 {padding-left: 0pt;counter-reset: c3 1; }
 #l10> li>*:first-child:before {counter-increment: c3; content: counter(c1, decimal)"."counter(c2, decimal)"."counter(c3, decimal)". "; color: black; font-family:Helvetica, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l10> li:first-child>*:first-child:before {counter-increment: c3 0;  }
 #l11 {padding-left: 0pt;counter-reset: c3 1; }
 #l11> li>*:first-child:before {counter-increment: c3; content: counter(c1, decimal)"."counter(c2, decimal)"."counter(c3, decimal)". "; color: black; font-family:Helvetica, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l11> li:first-child>*:first-child:before {counter-increment: c3 0;  }
 #l12 {padding-left: 0pt;counter-reset: g1 1; }
 #l12> li>*:first-child:before {counter-increment: g1; content: "("counter(g1, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l12> li:first-child>*:first-child:before {counter-increment: g1 0;  }
 #l13 {padding-left: 0pt;counter-reset: h1 1; }
 #l13> li>*:first-child:before {counter-increment: h1; content: "("counter(h1, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l13> li:first-child>*:first-child:before {counter-increment: h1 0;  }
 #l14 {padding-left: 0pt;counter-reset: c3 1; }
 #l14> li>*:first-child:before {counter-increment: c3; content: counter(c1, decimal)"."counter(c2, decimal)"."counter(c3, decimal)". "; color: black; font-family:Helvetica, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 #l14> li:first-child>*:first-child:before {counter-increment: c3 0;  }
 #l15 {padding-left: 0pt;counter-reset: i1 1; }
 #l15> li>*:first-child:before {counter-increment: i1; content: "("counter(i1, decimal)") "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 #l15> li:first-child>*:first-child:before {counter-increment: i1 0;  }
 table, tbody {vertical-align: top; overflow: visible; }
</style></head><body><h1 style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;line-height: 84%;text-align: left;">3D Virtual Worlds and the Metaverse: Current Status and Future Possibilities</h1><p class="s1" style="padding-top: 11pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">JOHN DAVID N. DIONISIO<span class="s2">, Loyola Marymount University</span></p><p class="s1" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">WILLIAM G. BURNS III<span class="s2">, Object Interoperability Lead for IEEE Virtual World Standard Group</span></p><p class="s1" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">RICHARD GILBERT<span class="s2">, Loyola Marymount University</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 5pt;text-indent: 0pt;line-height: 108%;text-align: justify;">Moving from a set of independent virtual worlds to an integrated network of 3D virtual worlds or Metaverse rests on progress in four areas: immersive realism, ubiquity of access and identity, interoperability, and scalability. For each area, the current status and needed developments in order to achieve a functional Metaverse are described. Factors that support the formation of a viable Metaverse, such as institutional and popular interest and ongoing improvements in hardware performance, and factors that constrain the achievement of this goal, including limits in computational methods and unrealized collaboration among virtual world stakeholders and developers, are also considered.</p><p class="s2" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;line-height: 108%;text-align: justify;">Categories and Subject Descriptors: C.2.4 [<b>Computer-Communication Networks</b>]: Distributed Sys- tems—<i>Distributed applications</i>; H.5.2 [<b>Information Interfaces and Presentation</b>]: User Interfaces— <i>Graphical user interfaces (GUI), Voice I/O</i>; I.3.7 [<b>Computer Graphics</b>]: Three-Dimensional Graphics and Realism—<i>Virtual reality, Animation</i></p><p class="s2" style="padding-left: 5pt;text-indent: 0pt;line-height: 14pt;text-align: justify;">General Terms: Algorithms, Design, Human Factors, Performance, Standardization Additional Key Words and Phrases: Immersion, Internet, second life, virtual worlds <b>ACM Reference Format:</b></p><p class="s2" style="padding-left: 5pt;text-indent: 0pt;line-height: 108%;text-align: justify;">Dionisio, J. D. N., Burns III, W. G., and Gilbert, R. 2013. 3D virtual worlds and the metaverse: Current status and future possibilities. ACM Comput. Surv. 45, 3, Article 34 (June 2013), 38 pages.</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="http://dx.doi.org/10.1145/2480741.2480751" class="a" target="_blank">DOI: </a><a href="http://dx.doi.org/10.1145/2480741.2480751" target="_blank">http://dx.doi.org/10.1145/2480741.2480751</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l1"><li><h2 style="padding-left: 16pt;text-indent: -11pt;text-align: left;">INTRODUCTION TO VIRTUAL WORLDS AND THE METAVERSE</h2><ol id="l2"><li><h2 style="padding-top: 3pt;padding-left: 24pt;text-indent: -18pt;text-align: left;">Definition and Significance of Virtual Worlds</h2><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Virtual worlds are persistent online computer-generated environments where multiple users in remote physical locations can interact in real time for the purposes of work or play. Virtual worlds constitute a subset of <i>virtual reality </i>applications, a more gen- eral term that refers to computer-generated simulations of three-dimensional objects or environments with seemingly real, direct, or physical user interaction. In 2008, the National Academy of Engineering (NAE) identified virtual reality as one of 14 Grand Challenges awaiting solutions in the 21st Century [National Academy of Engineering 2008]. Although the NAE Grand Challenges Committee has a predominantly Ameri- can perspective, it is informed by constituents with international backgrounds, with a</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="http://www.engineeringchallenges.org/" class="s5" target="_blank">U.K. member and several U.S.-based members who have close associations with Latin American, African, and Asian institutions (</a>http://www.engineeringchallenges.org/ cms/committee.aspx). In addition, as Zhao [2011] notes, a 2006 report by the Chinese</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="532" height="1" alt="image" src="2480741.2480751_files/Image_001.png"/></span></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="mailto:dondi@lmu.edu" class="a" target="_blank">Author’s addresses: J. D. N. Dionisio, Department of Electrical Engineering &amp; Computer Science, Loyola Marymount University; W. G. Burns III, IEEE Virtual World Standard Group; R. Gilbert, Department of Psychology, Loyola Marymount University; corresponding author’s (Dionisio); email: </a><a href="mailto:dondi@lmu.edu" target="_blank">dondi@lmu.edu.</a></p><p class="s6" style="text-indent: 0pt;line-height: 8pt;text-align: left;">+</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="mailto:permissions@acm.org" class="a" target="_blank">Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax 1 (212) 869-0481, or </a><a href="mailto:permissions@acm.org" target="_blank">permissions@acm.org.</a></p><p class="s7" style="text-indent: 0pt;line-height: 8pt;text-align: left;">Q</p><p style="text-indent: 0pt;text-align: left;"/><p class="s2" style="padding-left: 7pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">c 2013 ACM 0360-0300/2013/06-ART34 $15.00</p><p style="text-indent: 0pt;text-align: left;"><span><img width="47" height="96" alt="image" src="2480741.2480751_files/Image_002.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 12pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">34</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a href="http://dx.doi.org/10.1145/2480741.2480751" class="a" target="_blank">DOI: </a><a href="http://dx.doi.org/10.1145/2480741.2480751" target="_blank">http://dx.doi.org/10.1145/2480741.2480751</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s9" style="padding-top: 7pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">ACM Computing Surveys, Vol. 45, No. 3, Article 34, Publication date: June 2013.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">government entitled <i>Development Plan Outline for Medium and Long-Term Science and Technology Development (2006–2020) </i><a href="http://www.gov.cn/jrzg/2006-02/09/content_" class="s5" target="_blank">(</a>http://www.gov.cn/jrzg/2006-02/09/content_ 183787.htm) and a 2007 Japanese government report labeled <i>Innovation 2025 </i><a href="http://www.cao.go.jp/innovation/index.html)" class="s5" target="_blank">(http:// </a>www.cao.go.jp/innovation/index.html) both included virtual reality as a priority technology worthy of development. Thus, there is international support for promot- ing advances in virtual reality applications.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><h2 style="padding-left: 23pt;text-indent: -18pt;text-align: left;">Purpose and Scope of this Article</h2><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">This article surveys the current status of computing as it applies to 3D virtual spaces and outlines what is needed to move from a set of independent virtual worlds to an integrated network of 3D virtual worlds or <i>Metaverse </i>that constitutes a compelling alternative realm for human sociocultural interaction. In presenting this status report and roadmap for advancement, attention will be specifically directed to the following four features that are considered central components of a viable Metaverse.</p><ol id="l3"><li><p class="s4" style="padding-top: 7pt;padding-left: 24pt;text-indent: -17pt;text-align: justify;">Realism. <span class="p">Is the virtual space sufficiently realistic to enable users to feel psycholog- ically and emotionally immersed in the alternative realm?</span></p></li><li><p class="s4" style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">Ubiquity. <span class="p">Are the virtual spaces that comprise the Metaverse accessible through all existing digital devices (from desktops to tablets to mobile devices), and do the user’s virtual identities or collective persona remain intact throughout transitions within the Metaverse?</span></p></li><li><p class="s4" style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">Interoperability. <span class="p">Do the virtual spaces employ standards such that (a) digital as- sets used in the reconstruction or rendering of virtual environments remain in- terchangeable across specific implementations and (b) users can move seamlessly between locations without interruption in their immersive experience?</span></p></li><li><p class="s4" style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">Scalability. <span class="p">Does the server architecture deliver sufficient power to enable massive numbers of users to occupy the Metaverse without compromising the efficiency of the system and the experience of the users?</span></p></li></ol><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">In order to provide context for considering the present state and potential future of 3D virtual spaces, the article begins by presenting the historical development of virtual worlds and conceptions of the Metaverse. This history incorporates literary and gaming precursors to virtual world development as well as direct advances in virtual world technology, because these literary and gaming developments often preceded and significantly influenced later achievements in virtual world technology. Thus, they are most accurately treated as important elements in the technical development of 3D spaces rather than as unrelated cultural events.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><h2 style="padding-left: 16pt;text-indent: -11pt;text-align: left;">THE EVOLUTION OF VIRTUAL WORLDS</h2><ol id="l4"><li><h2 style="padding-top: 3pt;padding-left: 23pt;text-indent: -18pt;text-align: left;">Five Phases of Virtual World Development</h2><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: right;">The development of virtual worlds has a detailed history in which acts of literary imag- ination and gaming innovations have led to advances in open-ended, socially oriented virtual platforms (examples of narratives include those by Au [2008], Boellstorff [2008], and Ludlow and Wallace [2007]). This history can be broadly divided into five phases. In the initial phase, beginning in the late 1970s, text-based virtual worlds emerged in two varieties. MUDs, or multi-user dungeons, involved the creation of fantastic realities that resembled Tolkien’s <i>Lord of the Rings </i>or the role-playing dice game <i>Dungeons and Dragons</i>. MUSHs, or multi-user shared hallucinations, consisted of less-defined exploratory spaces where educators experimented with the possibility of collaborative</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">creation [Turkle 1995].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The next phase of development occurred around a decade later when Lucasfilm, partly inspired by the 1984 publication of William Gibson’s <i>Neuromancer</i>, introduced</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Habitat for the Commodore 64 in 1986 and the Fujitsu platform in 1989. Habitat was the first high-profile commercial application of virtual world technology and the first virtual world to incorporate a graphical interface. This initial graphical interface was in 2D versus 3D, however, and the online environment appeared to the user as something analogous to a primitive cartoon operating at slow speeds via dial-up modems. Habitat was also the first virtual world to employ the term “avatar” (from the Sanskrit meaning “the deliberate appearance or manifestation of a deity on earth”) to describe its digital residents or inhabitants. As with the original Sanskrit, the contemporary usage of the term avatar involves a transposition of consciousness into a new form. However, in contrast to the ancient usage, the modern transition in form involves movement from a human body to a digital representation rather than from a god to a man or woman. Pioneering work in virtual reality systems and user interfaces mark the transition from this phase of development to the next [Blanchard et al. 1990; Cruz-Neira et al. 1992; Lanier 1992; Krueger 1993].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The third phase of development, beginning in the mid-1990s, was a particularly vibrant one with advances in computing power and graphics underlying progress in several major areas, including the introduction of user-created content, 3D graphics, open-ended socialization, and integrated audio. In 1994, Web World offered a 2.5D (iso- metric) world that provided users with open-ended building capability for the first time. The incorporation of user-based content-creation tools ushered in a paradigm shift from precreated virtual settings to online environments that were contributed to, changed, and built by participants in real time. In 1995, Worlds, Inc. (http://www.worlds.com) became the first publicly available virtual world with full three-dimensional graphics. Worlds, Inc. also revived the open-ended non-game-based genre (originally found in text-based MUSHs) by enabling users to socialize in 3D spaces, thus moving virtual worlds further away from a gaming model toward an emphasis on providing an alter- native setting or culture to express the full range and complexity of human behavior. In this way, the scope and diversity of activities in virtual worlds came to parallel that of the Internet as a whole, only in 3D and with corresponding modes of interaction. 1995 also saw the introduction of Activeworlds (http://www.activeworlds.com), a virtual world based entirely on the vision expressed in Neil Stephenson’s 1992 novel <i>Snow Crash</i>. Activeworlds explicitly expected users to personalize and co-construct a full 3D virtual environment. Finally, in 1996, OnLive! Traveler became the first publicly available 3D virtual environment to include natively utilized spatial voice chat and movement of avatar lips via processing phonemes.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The fourth phase of development occurred during the postmillennial decade. This period was characterized by dramatic expansion in the user base of commercial vir- tual worlds (such as Second Life), enhanced in-world content creation tools, increased involvement of major institutions from the physical world (e.g., corporations, col- leges and universities, and nonprofit organizations), the development of an advanced virtual economy, and gradual improvements in graphical fidelity. Avatar Reality’s Blue Mars, released in 2009, involved the most ambitious attempt to incorporate a much higher level of graphical realism into virtual worlds by using the then state-of- the-art CryEngine 2, which was originally developed by Crytek for gaming applications. Avatar Reality also allowed 3D objects to be created out-of-world as long as they could be saved in one of several open formats. Unfortunately, the effort to achieve greater graphical realism significantly raised the system requirements for client machines to a degree that was not cost-effective for Blue Mars’s target user base, and this overreach- ing necessitated massive restructuring/scaling down of a once-promising initiative.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Overlapping the introduction of Second Life and Blue Mars has been a fifth phase of development. This phase, which began in 2007 and remains ongoing, involves open- source decentralized contributions to the development of 3D virtual worlds. Solipsis</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 4pt;padding-left: 78pt;text-indent: 0pt;text-align: left;">Table I. Major Literary/Narrative Influences to Virtual World Technology</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:5.677pt" cellspacing="0"><tr style="height:12pt"><td style="width:51pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s11" style="padding-left: 11pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Y<span class="s12">EAR</span>(<span class="s12">S</span>)</p></td><td style="width:100pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s11" style="padding-left: 5pt;padding-right: 5pt;text-indent: 0pt;line-height: 9pt;text-align: center;">L<span class="s12">ITERARY</span>/N<span class="s12">ARRATIVE</span></p></td><td style="width:243pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s11" style="padding-left: 98pt;padding-right: 97pt;text-indent: 0pt;line-height: 9pt;text-align: center;">S<span class="s12">IGNIFICANCE</span></p></td></tr><tr style="height:36pt"><td style="width:51pt;border-top-style:solid;border-top-width:1pt"><p class="s11" style="padding-top: 5pt;padding-right: 6pt;text-indent: 0pt;text-align: right;">1955/1966</p></td><td style="width:100pt;border-top-style:solid;border-top-width:1pt"><p class="s13" style="padding-top: 5pt;padding-left: 5pt;padding-right: 5pt;text-indent: 0pt;text-align: center;">Lord of the Rings</p></td><td style="width:243pt;border-top-style:solid;border-top-width:1pt"><p class="s11" style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Tolkien’s foundational work of 20th-century fantasy literature serves as a source of inspiration for many games and virtual world environments.</p></td></tr><tr style="height:24pt"><td style="width:51pt"><p class="s11" style="padding-top: 2pt;padding-right: 6pt;text-indent: 0pt;text-align: right;">1974</p></td><td style="width:100pt"><p class="s13" style="padding-top: 2pt;padding-left: 5pt;padding-right: 5pt;text-indent: 0pt;text-align: center;">Dungeons and Dragons</p></td><td style="width:243pt"><p class="s11" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Originally designed by Gary Gygax and Dave Arneson. Widely regarded as the beginning of modern role-playing games.</p></td></tr><tr style="height:33pt"><td style="width:51pt"><p class="s11" style="padding-top: 2pt;padding-right: 6pt;text-indent: 0pt;text-align: right;">1981</p></td><td style="width:100pt"><p class="s13" style="padding-top: 2pt;padding-left: 5pt;padding-right: 5pt;text-indent: 0pt;text-align: center;">True Names</p></td><td style="width:243pt"><p class="s11" style="padding-top: 2pt;padding-left: 7pt;padding-right: 23pt;text-indent: 0pt;text-align: left;">Vernor Vinge’s science fiction novella presents a fully fleshed-out version of cyberspace. Influences later classics such as <i>Neuromancer </i>and <i>Snow Crash</i>.</p></td></tr><tr style="height:24pt"><td style="width:51pt"><p class="s11" style="padding-top: 2pt;padding-right: 6pt;text-indent: 0pt;text-align: right;">1984</p></td><td style="width:100pt"><p class="s13" style="padding-top: 2pt;padding-left: 5pt;padding-right: 5pt;text-indent: 0pt;text-align: center;">Neuromancer</p></td><td style="width:243pt"><p class="s11" style="padding-top: 2pt;padding-left: 6pt;padding-right: 19pt;text-indent: 0pt;text-align: left;">William Gibson’s seminal cyberpunk novel popularized the early concept of cyberspace as “the Matrix.”</p></td></tr><tr style="height:32pt"><td style="width:51pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s11" style="padding-top: 2pt;padding-right: 6pt;text-indent: 0pt;text-align: right;">1992</p></td><td style="width:100pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s13" style="padding-top: 2pt;padding-left: 5pt;padding-right: 5pt;text-indent: 0pt;text-align: center;">Snow Crash</p></td><td style="width:243pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s11" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">The term <i>Metaverse </i>was coined in Neal Stephenson’s science fiction novel to describe a virtual reality-based successor to the Internet.</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">is notable not only as one of the first open-source virtual world systems but also for its proposed peer-to-peer architecture [Keller and Simon 2002; Frey et al. 2008]. It has since been followed by a host of other open-source projects, including Open Cobalt [Open Cobalt Project 2011], Open Wonderland [Open Wonderland Foundation 2011], and OpenSimulator [OpenSimulator Project 2011].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Decentralized development has led to the decoupling of the client and server sides of a virtual world system, facilitated by convergence on the network protocol used by Linden Lab for Second Life as a de facto standard. The open-source Imprudence/Kokua viewer marked the first “third-party viewer” for Second Life [Antonelli (AV) and Maxsted (AV) 2011],<span class="s14">1</span><span class="s15"> </span>an application category that grew to include projects such as Phoenix (and later Firestorm) [Lyon (AV) 2011], Singularity [Gearz (AV) 2011], Kirstens [Cinquetti (AV) 2011], and Nirans (itself derived from Kirstens) [Dean (AV) 2011]. On the server side, the aforementioned OpenSimulator has emerged as a Second Life workalike and al- ternative and has itself spawned forks, such as Aurora-Sim [Aurora-Sim Project 2011] and realXtend [realXtend Association 2011]. The Second Life protocol itself remains proprietary under the full control of Linden Lab.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: right;">The emergent multiplicity of interoperable clients (viewers) and servers forms the basis of our fifth phase’s ultimate endpoint: full interoperability and interchangeability across virtual world servers and clients, in the same way that the Worldwide Web consists of multiple client (browser) and server options centered around the standard HTTP(S) protocol. Open source availability has facilitated new integration possibilities, such as cloud-computing virtual world hosts and authentication using social network credentials [Trattner et al. 2010; Korolov 2011]. With activities such as these and continuing work on viewers and servers, it can be said that we have entered an open development phase for virtual worlds. Per Eric S. Raymond’s well-known metaphor, the virtual worlds cathedral has evolved into a virtual worlds bazaar [Raymond 2001]. Tables I to III summarize the major literary/narrative influences to virtual world development and the advances in virtual world technology that have occurred during</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">the 30-plus years since MUDs and MUSHs were introduced.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="48" height="0" alt="image" src="2480741.2480751_files/Image_003.png"/></span></p><p class="s16" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">1<span class="s17">When</span><span class="s2"> individuals use an avatar name as opposed to their physical-world name for a paper or software product, we have added a parenthetical (AV), for avatar, after that name to signify this choice. This conforms to the real-world convention of citing an author’s chosen pseudonym (e.g., Mark Twain) rather than his or her given name (e.g., Samuel Clemens). Any author name that is not followed by an (AV) references the author’s physical-world name.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 4pt;padding-left: 21pt;text-indent: 0pt;text-align: center;">Table II. Major 20th-Century Advances in Virtual World Technology</p><p style="text-indent: 0pt;text-align: left;"><span><img width="526" height="0" alt="image" src="2480741.2480751_files/Image_004.png"/></span></p><p class="s2" style="padding-top: 4pt;padding-left: 21pt;text-indent: 92pt;line-height: 120%;text-align: left;">Phase I: Text-Based Virtual Worlds—Late 1970s Y<span class="s18">EARS                         </span>E<span class="s18">VENT</span>/M<span class="s18">ILESTONE                                                                              </span>S<span class="s18">IGNIFICANCE</span></p><table style="border-collapse:collapse;margin-left:5.479pt" cellspacing="0"><tr style="height:15pt"><td style="width:58pt;border-top-style:solid;border-top-width:1pt"><p class="s11" style="padding-top: 5pt;padding-left: 29pt;text-indent: 0pt;line-height: 8pt;text-align: left;">1979</p></td><td style="width:97pt;border-top-style:solid;border-top-width:1pt"><p class="s13" style="padding-top: 5pt;padding-left: 10pt;text-indent: 0pt;line-height: 8pt;text-align: left;">MUDs and MUSHes</p></td><td style="width:239pt;border-top-style:solid;border-top-width:1pt"><p class="s11" style="padding-top: 5pt;padding-left: 10pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Roy Trubshaw and Richard Bartle complete the first</p></td></tr><tr style="height:9pt"><td style="width:58pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:97pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:239pt"><p class="s13" style="padding-left: 10pt;text-indent: 0pt;line-height: 8pt;text-align: left;">multi-user dungeon <span class="s11">or </span>multi-user domain<span class="s11">—a multiplayer,</span></p></td></tr><tr style="height:9pt"><td style="width:58pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:97pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:239pt"><p class="s11" style="padding-left: 10pt;text-indent: 0pt;line-height: 8pt;text-align: left;">real-time, virtual gaming world described primarily in text.</p></td></tr><tr style="height:9pt"><td style="width:58pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:97pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:239pt"><p class="s11" style="padding-left: 10pt;text-indent: 0pt;line-height: 8pt;text-align: left;">MUSHes, text-based online environments in which multiple</p></td></tr><tr style="height:9pt"><td style="width:58pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:97pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:239pt"><p class="s11" style="padding-left: 10pt;text-indent: 0pt;line-height: 8pt;text-align: left;">users are connected at the same time for open-ended</p></td></tr><tr style="height:9pt"><td style="width:58pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:97pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:239pt"><p class="s11" style="padding-left: 10pt;text-indent: 0pt;line-height: 8pt;text-align: left;">socialization and collaborative work, also appear at this</p></td></tr><tr style="height:10pt"><td style="width:58pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:97pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:239pt"><p class="s11" style="padding-left: 10pt;text-indent: 0pt;line-height: 8pt;text-align: left;">time.</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><span><img width="526" height="0" alt="image" src="2480741.2480751_files/Image_005.png"/></span></p><p class="s2" style="padding-top: 7pt;padding-left: 21pt;text-indent: 0pt;text-align: center;">Phase II: Graphical Interface and Commercial Application—1980s</p><p class="s2" style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: right;">1986/1989   <i>Habitat for Commodore</i></p><p class="s3" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: right;">64 (1986) and Fujitsu</p><p class="s3" style="padding-left: 83pt;text-indent: 0pt;line-height: 9pt;text-align: left;">platform (1989)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: right;">early 1990s        <i>Reality Built For Two,</i></p><p class="s3" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: right;">CAVE, Artificial Reality</p><p class="s2" style="padding-top: 7pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">Partly inspired by William Gibson’s <i>Neuromancer</i>. The first commercial simulated multi-user environment using 2D graphical representations and employing the term “avatar,” borrowed from the Sanskrit term meaning the deliberate appearance or manifestation of a deity in human form.</p><p class="s2" style="padding-top: 7pt;padding-left: 10pt;text-indent: 0pt;text-align: left;">Prototype virtual reality systems and immersive environments begin to proliferate.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-top: 5pt;padding-bottom: 1pt;padding-left: 159pt;text-indent: -131pt;text-align: left;">Phase III: User-Created Content, 3D Graphics, Open-Ended Socialization, Integrated Audio—1990s</p><table style="border-collapse:collapse;margin-left:5.479pt" cellspacing="0"><tr style="height:44pt"><td style="width:62pt;border-top-style:solid;border-top-width:1pt"><p class="s11" style="padding-top: 5pt;padding-right: 13pt;text-indent: 0pt;text-align: right;">1994</p></td><td style="width:90pt;border-top-style:solid;border-top-width:1pt"><p class="s13" style="padding-top: 5pt;padding-left: 13pt;padding-right: 13pt;text-indent: 0pt;text-align: center;">Web World</p></td><td style="width:242pt;border-top-style:solid;border-top-width:1pt"><p class="s11" style="padding-top: 5pt;padding-left: 14pt;padding-right: 5pt;text-indent: 0pt;text-align: left;">The first 2.5D (isometric) world where tens of thousands could chat, build, and travel. Initiated a paradigm shift from precreated environments to environments contributed to, changed, and built by participants in real time.</p></td></tr><tr style="height:60pt"><td style="width:62pt"><p class="s11" style="padding-top: 2pt;padding-right: 13pt;text-indent: 0pt;text-align: right;">1995</p></td><td style="width:90pt"><p class="s13" style="padding-top: 2pt;padding-left: 13pt;padding-right: 13pt;text-indent: 0pt;text-align: center;">Worlds Inc.</p></td><td style="width:242pt"><p class="s11" style="padding-top: 2pt;padding-left: 14pt;padding-right: 6pt;text-indent: 0pt;text-align: left;">One of the first publicly available 3D virtual user environments. Enhanced the open-ended non-game-based genre by enabling users to socialize in 3D spaces. Continued moving virtual worlds away from a gaming model toward an emphasis on providing an alternative setting or culture to express the full range and complexity of human behavior.</p></td></tr><tr style="height:42pt"><td style="width:62pt"><p class="s11" style="padding-top: 2pt;padding-right: 13pt;text-indent: 0pt;text-align: right;">1995</p></td><td style="width:90pt"><p class="s13" style="padding-top: 2pt;padding-left: 13pt;padding-right: 13pt;text-indent: 0pt;text-align: center;">Activeworlds</p></td><td style="width:242pt"><p class="s11" style="padding-top: 2pt;padding-left: 14pt;padding-right: 5pt;text-indent: 0pt;text-align: left;">Based entirely on <i>Snow Crash</i>, popularized the project of creating an actual Metaverse. Offered basic content-creation tools to personalize and co-construct the virtual environment.</p></td></tr><tr style="height:32pt"><td style="width:62pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s11" style="padding-top: 2pt;padding-right: 13pt;text-indent: 0pt;text-align: right;">1996</p></td><td style="width:90pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s13" style="padding-top: 2pt;padding-left: 13pt;padding-right: 13pt;text-indent: 0pt;text-align: center;">OnLive! Traveler</p></td><td style="width:242pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s11" style="padding-top: 2pt;padding-left: 14pt;padding-right: 5pt;text-indent: 0pt;text-align: left;">The first publicly available system that natively utilized spatial voice chat and incorporated the movement of avatar lips via processing phonemes.</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: left;">Reflecting these developments, Gilbert [2011] has identified five essential features that characterize a contemporary state-of-the art virtual world.</p><ol id="l5"><li><p style="padding-top: 7pt;padding-left: 24pt;text-indent: -17pt;text-align: justify;">It has a 3D graphical interface and integrated audio. An environment with a text interface alone does not constitute an advanced virtual world.</p></li><li><p style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">It supports massively multi-user remote interactivity. Simultaneous interactivity among large numbers of users in remote physical locations is a minimum require- ment, not an advanced feature.</p></li><li><p style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">It is persistent. The virtual environment continues to operate even when a partic- ular user is not connected.</p></li><li><p style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">It is immersive. The environment’s level of spatial, environmental, and multisen- sory realism creates a sense of psychological presence. Users have a sense of “being inside,” “inhabiting,” or “residing within” the digital environment rather than being outside of it, thus intensifying their psychological experience.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 4pt;padding-bottom: 4pt;padding-left: 84pt;text-indent: 0pt;text-align: left;">Table III. Major 21st-Century Advances in Virtual World Technology</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="530" height="0" alt="image" src="2480741.2480751_files/Image_006.png"/></span></p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;text-align: center;">Phase IV: Major Expansion in Commercial Virtual World User Bases, Enhanced Content Creation Tools, Increased Institutional Presence, Development of Robust Virtual Economy, Improvements in Graphical Fidelity—Postmillennial Decade</p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="530" height="0" alt="image" src="2480741.2480751_files/Image_007.png"/></span></p><p class="s2" style="padding-left: 32pt;text-indent: 0pt;text-align: left;">Y<span class="s18">EARS                          </span>E<span class="s18">VENT</span>/M<span class="s18">ILESTONE                                                                      </span>S<span class="s18">IGNIFICANCE</span></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 2pt;text-align: left;"><span><img width="519" height="3" alt="image" src="2480741.2480751_files/Image_008.png"/></span></p><p class="s2" style="padding-top: 5pt;padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: left;">2003–present      <i>Second Life            </i>Popular open-ended commercial virtual environment with</p><ol id="l6"><li><p class="s2" style="padding-left: 170pt;text-indent: 0pt;text-align: left;">in-world live editing, (2) ability to import externally created 3D objects into the virtual environment, and</p><p class="s2" style="padding-left: 170pt;text-indent: 0pt;text-align: left;">(3) advanced virtual economy. Primary virtual world for corporate and educational institutions.</p><p class="s2" style="padding-top: 5pt;padding-left: 24pt;text-indent: 0pt;line-height: 9pt;text-align: left;">2009–present    <i>Avatar Reality/</i></p><p class="s3" style="padding-left: 103pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Blue Mars</p><p class="s2" style="padding-top: 5pt;padding-left: 16pt;text-indent: 0pt;text-align: left;">A closed-source foray into much higher graphical realism using 3D graphics engine technology initially developed in the gaming industry.</p><p class="s2" style="padding-top: 7pt;padding-bottom: 1pt;padding-left: 72pt;text-indent: 0pt;text-align: left;">Phase V: Open Decentralized Development—2007 and Beyond</p><table style="border-collapse:collapse;margin-left:6.028pt" cellspacing="0"><tr style="height:53pt"><td style="width:74pt;border-top-style:solid;border-top-width:1pt"><p class="s11" style="padding-top: 5pt;padding-right: 5pt;text-indent: 0pt;text-align: right;">2007</p></td><td style="width:84pt;border-top-style:solid;border-top-width:1pt"><p class="s13" style="padding-top: 5pt;padding-left: 4pt;padding-right: 4pt;text-indent: 0pt;text-align: center;">Solipsis</p></td><td style="width:235pt;border-top-style:solid;border-top-width:1pt"><p class="s11" style="padding-top: 5pt;padding-left: 5pt;padding-right: 10pt;text-indent: 0pt;text-align: left;">The first open-source decentralized virtual world system. Open-source development theoretically allowed new server and client variants to be created from the original code base—something that would actually become a reality with systems other than Solipsis.</p></td></tr><tr style="height:42pt"><td style="width:74pt"><p class="s11" style="padding-top: 2pt;padding-right: 5pt;text-indent: 0pt;text-align: right;">2008</p></td><td style="width:84pt"><p class="s13" style="padding-top: 2pt;padding-left: 4pt;padding-right: 4pt;text-indent: 0pt;text-align: center;">Imprudence/Kokua</p></td><td style="width:235pt"><p class="s11" style="padding-top: 2pt;padding-left: 5pt;padding-right: 17pt;text-indent: 0pt;text-align: left;">One of the earliest alternative open-source viewers for an existing virtual world server (Second Life). So-called</p><p class="s11" style="padding-left: 5pt;padding-right: 10pt;text-indent: 0pt;text-align: left;">“third-party viewers” for Second Life have proliferated ever since.</p></td></tr><tr style="height:60pt"><td style="width:74pt"><p class="s11" style="padding-top: 2pt;padding-right: 5pt;text-indent: 0pt;text-align: right;">2009</p></td><td style="width:84pt"><p class="s13" style="padding-top: 2pt;padding-left: 4pt;padding-right: 4pt;text-indent: 0pt;text-align: center;">OpenSimulator</p></td><td style="width:235pt"><p class="s11" style="padding-top: 2pt;padding-left: 5pt;padding-right: 6pt;text-indent: 0pt;text-align: left;">First instance of multiple servers following the same virtual world protocol (Second Life), later accompanied by a choice of multiple viewers that use this protocol. Although the Second Life protocol has become a de facto standard, the protocol itself remains proprietary and occasionally requires reverse engineering.</p></td></tr><tr style="height:77pt"><td style="width:74pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s11" style="padding-top: 2pt;padding-right: 5pt;text-indent: 0pt;text-align: right;">2010 and beyond</p></td><td style="width:84pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s13" style="padding-top: 2pt;padding-left: 12pt;padding-right: 7pt;text-indent: -4pt;text-align: left;">Open Development of the Metaverse</p></td><td style="width:235pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s11" style="padding-top: 2pt;padding-left: 5pt;padding-right: 10pt;text-indent: 0pt;text-align: left;">Interoperability and interchangeability across servers and clients through standard virtual world protocols, formats, and digital credentials—users and providers can choose among virtual world client and server implementations respectively without worrying about compatibility or authentication, just as today’s Web runs multiple Web servers and browsers with standards, such as OpenID and OAuth.</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><p style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">It emphasizes user-generated activities and goals and provides content creation tools for personalization of the virtual environment and experience. In contrast to immersive games, where goals—such as amassing points, overcoming an enemy, or fulfilling a quest—are built into the program, virtual worlds provide a more open-ended setting where, similar to physical life and culture, users can define and implement their own activities and goals. However, there are some immer- sive games, such as World of Warcraft, that include specific goals for the users but are still psychologically and socially complex. These intricate immersive environ- ments stand at the border of games and virtual worlds.</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><h2 style="padding-left: 23pt;text-indent: -18pt;text-align: left;">From Individual Virtual Worlds to the Metaverse</h2><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">The word Metaverse is a portmanteau of the prefix “meta” (meaning “beyond”) and the suffix “verse” (shorthand for “universe”). Thus it literally means a universe beyond the physical world. More specifically this “universe beyond” refers to a computer-generated world, distinguishing it from metaphysical or spiritual conceptions of domains beyond</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: right;">the physical realm. In addition, the Metaverse refers to a fully immersive three- dimensional digital environment in contrast to the more inclusive concept of <i>cyberspace </i>that reflects the totality of shared online space across all dimensions of representation. Although the Metaverse always references an immersive three-dimensional digital space, conceptions about its specific nature and organization have changed over time. The general progression has been from viewing the Metaverse as an amplified version of an individual virtual world to conceiving it as a large network of interconnected virtual worlds. Neal Stephenson, who coined the term in his 1992 novel <i>Snow Crash</i>, vividly conveyed the <i>Metaverse as Virtual World </i>perspective. In Stephenson’s concep- tion of the Metaverse, humans-as-avatars interact with intelligent agents and each other in an immersive world that appears as a nighttime metropolis developed along a neon-lit, hundred-meter-wide grand boulevard called the Street, evoking images of an exaggerated Las Vegas strip. The Street runs the entire circumference of a feature- less black planet considerably larger than Earth that has been visited by 120 million users, approximately 15 million of whom occupy the Street at a given time. Users gain access to the Metaverse through computer terminals that project a first-person per- spective virtual reality display onto goggles and pump stereo digital sound into small earphones that drop from the bows of the goggles and plug into the user’s ears. Users have the ability to customize their avatars with the sole restriction of height (to avoid mile-high avatars), to travel by walking or by virtual vehicle, to build structures on acquired parcels of virtual real estate, and to engage in the full range of human social and instrumental activities. Thus, the Metaverse that Stephenson brilliantly imagined is, in both form and operation, essentially an extremely large and heavily populated virtual world that operates, not as a gaming environment with specific parameters and goals, but as an open-ended digital culture that operates in parallel with the physical</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">domain.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Since Stephenson’s novel appeared, technological advances have enabled real-life implementation of virtual worlds and more complex and expansive conceptions of the Metaverse have developed. In 2007, the Metaverse Roadmap Project [Smart et al. 2007] offered a multifaceted conception of the Metaverse that involved both “simu- lation technologies that create physically persistent virtual spaces such as virtual and mirror worlds” and “technologies that virtually-enhance physical reality such as augmented reality” (i.e., technologies that connect networked information and compu- tational intelligence to physical objects and spaces). Although this effort is notable in its attempt to view the Metaverse in broader terms than an individual virtual world and is itself advancing quite rapidly (but then again what technology isn’t), the in- clusion of augmented reality technologies served to redirect attention from the core qualities of immersion, three-dimensionality, and simulation that are the foundations of virtual world environments. We consider the augmented reality space to be a subset of the Metaverse that constitutes a crossroads between purely virtual environments and purely real or visceral environments. Like any virtual world system, augmented reality constructs also access assets and data from a self-contained or shared world state, overlaying them on a view of the physical world rather than a synthetic one.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">In contrast to the Metaverse Roadmap, a 2008 white paper on Solipsis, an open- source architecture for creating large systems of virtual environments using a peer- to-peer topology, provided the first published account of the contemporary <i>Metaverse as Network of Virtual Worlds </i>perspective. The Solipsis white paper defined the con- cept as “a massive infrastructure of inter-linked virtual worlds accessible via a com- mon user interface (browser) and incorporating both 2D and 3D in an Immersive Internet” [Frey et al. 2008]. Frey et al. and the IEEE Virtual World Standard Group (http://www.metaversestandards.org) also offered a clear developmental progression from an individual virtual world to the Metaverse using concepts and terminology</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">aligned with the organization of the physical universe [Burns 2010; IEEE VW Stan- dard Working Group 2011b]. This progression starts with separate virtual worlds or <i>MetaWorlds </i>(analogous to individual physical planets) with no interworld tran- sit capabilities (e.g., Second Life, Entropia Universe, and the Chinese virtual world of Hipihi). <i>MetaGalaxies </i>(sometimes referred to as <i>hypergrids</i>) then involve multiple virtual worlds clustered together as perceived collectives under a single authority. MetaGalaxies, such as Activeworlds and OpenSim Hypergrid-enabled virtual environ- ments, permit perceived teleportation or space travel. In such environments, users have the perception of leaving one “planet” or perceived contiguous space and arriving at another. The progression of development culminates in a complete Metaverse that involves multiple MetaGalaxies and MetaWorld systems. A standardized protocol and set of abilities would allow users to move between virtual worlds in a seamless manner regardless of the controlling entity for any particular virtual region.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><h2 style="padding-left: 23pt;text-indent: -18pt;text-align: left;">Computational Advances and the Metaverse</h2><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Although there are multiple virtual worlds currently in existence and arguably one hy- pergrid or MetaGalaxy in Activeworlds, the Metaverse itself remains a concept awaiting full implementation. As noted in Section 1.2, the creation of a fully realized Metaverse will rest on continued progress with regard to four essential features of virtual world technology: psychological realism, ubiquity of access and identity, interoperability of content and experience across virtual environments, and scalability. In the section to follow, the current status of each of these features will be presented along with develop- ments in each area that could contribute to the attainment of a viable psychologically compelling Metaverse.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">In considering this framework for advancing virtual world technology, it is impor- tant to note two qualifications. First, in the interest of clarity of organization and presentation, the four features of the Metaverse will be mainly presented as separate and distinct variables even though overlap exists among these factors (e.g., increased graphical realism will have implications for server requirements and scalability).</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Second, it is important to acknowledge the likelihood that rapid changes in computing (as popularized by Moore’s law and Kurzweil’s law of accelerating returns [Moore 1965; Kurzweil 2001]) will impact the currency of some aspects of this survey. Thus, examples will be cited from time to time for illustrative purposes with full knowledge that the time it takes to prepare, review, and disseminate this document will render some of these examples outdated or obsolete. Nevertheless, although many specifics that make up the virtual realm are subject to rapid change, the general principles and computing issues related to the formation of a fully operational Metaverse that are the primary focus of the present work will continue to be relevant over time [Lanier and Biocca 1992; National Academy of Engineering 2008; Zhao 2011].</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><h2 style="padding-left: 16pt;text-indent: -11pt;text-align: left;">FEATURES OF THE METAVERSE: CURRENT STATUS AND FUTURE POSSIBILITIES</h2><ol id="l7"><li><h2 style="padding-top: 2pt;padding-left: 23pt;text-indent: -18pt;text-align: left;">Realism</h2><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">We immediately qualify our use of the term <i>realism </i>to mean, in this context, <i>immersive </i>realism. In the same way that realism in cinematic computer-generated imagery is qualified by its believability rather than devotion to detail (though certainly a sufficient degree of detail is expected), realism in the Metaverse is sought in the service of a user’s psychological and emotional engagement within the environment. A virtual environment is perceived as more realistic based on the degree to which it transports a user into that environment and on the transparency of the boundary between the user’s physical actions and those of his or her avatar. By this token, virtual world realism is not purely additive nor, visually speaking, strictly photographic: in many cases, strategic</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">rendering choices can yield better returns than merely adding polygons, pixels, objects, or bits in general across the board.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">What does remain constant across all perspectives on realism is the <i>instrumentation </i>through which human beings interact with the environment, that is, their senses and their bodies, particularly through their faces and hands. We thus approach realism through this structure of perception and expression.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">We treat the subject at a historical and survey level in this article, examining current and future trends. Greater technical detail and focus solely on the (perceived) realism of virtual worlds can be found in the cited work and tutorial materials, such as Glencross et al. [2006].</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l8"><li><p class="s19" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Sight. <span class="p">Not surprisingly, sight and visuals comprise the sensory channel with the most extensive history within virtual worlds. As described in Section 2, the earliest visual medium for virtual worlds—and for all of computing for that matter—was plain text. Text was used to form imagery within the mind’s eye and, to a certain degree, it remains a very effective mechanism for doing so. In the end, however, words and symbols are indirect: they describe a world and leave specifics to individuals. Effective visual immersion involves eliminating this level of indirection—a virtual world’s visual presentation seeks to be as information-rich to our eyes as the real world is. The brain then recognizes imagery rather than synthesizing or recalling it (as it would when reading text).</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">To this end, the state of the art in visual immersion has until recently hewn very closely to the state of the art in real-time computer graphics. The precise meaning of real-time visual perception is complicated and fraught with nuances and external factors, such as attention, fixation, and nonvisual cues [Hayhoe et al. 2002; Recanzone 2003; Chow et al. 2006]. For this discussion, “real-time” refers to the simplified metric of frame rate, or the frequency with which a computer graphics system can render a scene. As graphics hardware and algorithms have pushed the threshold of what can be computed and displayed at 30 or more frames per second—a typically accepted minimum frame rate for real-time perception [Kumar et al. 2008; Liu et al. 2010]— virtual world applications have provided increasingly detailed visuals commensurate with the techniques of the time. Thus, alongside games, 3D modeling software, and to a certain extent 3D cinematic animation (qualified as such because the real-time constraint is lifted for the final product), virtual worlds have seen a progression of visual detail from flat polygons to smooth shading and texture mapping and finally to programmable shaders, which can apply transformations and other computations to graphical elements with great efficiency and flexibility.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Initially, visual richness was accomplished simply by increasing the amount of data used to render a scene: more and smaller polygons, higher-resolution textures, and interpolation techniques, such as anti-aliasing and smooth shading. The fixed-function graphics pipeline, so named because it processed 3D models in a uniform manner, fa- cilitated the stabilization of graphics libraries and incremental performance improve- ments in graphics hardware, but it also restricted the options available for delivering detailed objects and realistic rendering [Olano et al. 2004; Olano and Lastra 1998]. This served the area well for a time as hardware improvements accommodated the increasing bandwidth and standards emerged for representing and storing 3D models. Ultimately, however, using increased data for increased detail resulted in diminishing returns, as the cost of developing the information (3D models, 2D textures, animation rigging) can easily surpass the benefit seen in visual realism.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The arrival of programmable shaders allowed key portions of the computer graph- ics rendering pipeline to be written in a specialized language, resulting in unprece- dented flexibility. Shaders and shader languages also deliver great efficiency thanks</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">to accompanying hardware that is highly optimized and frequently parallelized specif- ically for them. Their eventual establishment as a new functional baseline for 3D libraries, such as OpenGL, represented a significant step for virtual world visuals, as it did for computer graphics applications in general [Rost et al. 2009]. With pro- grammable shaders, many aspects of 3D models—whether in terms of their geometry (vertex shaders) or their final image rendering (fragment shaders)—became expressible algorithmically, thus eliminating the need for larger numbers of polygons or textures while at the same time sharply expanding the variety and flexibility with which ob- jects can be rendered or presented. This degree of flexibility has produced compelling real-time techniques for a variety of object and scene types. The separation of vertex and fragment shaders has also facilitated the augmentation of traditional approaches involving geometry and object models with image-space techniques [Roh and Lee 2005; Shah 2007].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Some of these techniques can see immediate applicability in virtual world environ- ments, as they involve exterior environments, such as terrain, lighting, bodies of water, precipitation, forests, fields, and the atmosphere [Hu et al. 2006; Wang et al. 2006; Bouthors et al. 2008; Boulanger et al. 2009; Seng and Wang 2009; Elek and Kmoch 2010] or common objects, materials, or phenomena, such as clothing, reflected light, smoke, translucence, or gloss [Adabala et al. 2003; Lewinski 2011; Sun and Mukherjee 2006; Ghosh 2007; Shah et al. 2009]. Techniques for specialized applications, such as surgery simulation [Hao et al. 2009] and radioactive threat training [Koepnick et al. 2010], have a place as well, because virtual world environments strive to be as open-ended as possible with minimal restrictions on the types of activities that can be performed within them (just as in the real world).</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Most virtual world client applications or <i>viewers </i>currently stand in transition be- tween fixed function rendering (explicit polygon representations, texture maps for small details) and programmable shaders (particle generation, bump mapping, at- mosphere and lighting, water) [Linden Lab 2011a]. Virtual world viewers tend to lag behind other graphics applications such as games, visual effects, or 3D modeling. Real- time lighting and shadows, for instance, were not implemented in the official Second Life viewer until June 2011, long after these had become fairly standard in other graphics application categories [Linden Lab 2011b].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The relative dominance of Second Life as a virtual world platform has resulted in an ecology of viewers that can all connect to the same Second Life environment but do so with varying features and functionalities, some of which pertain to visual detail [Antonelli (AV) and Maxsted (AV) 2011; Gearz (AV) 2011; Lyon (AV) 2011].<span class="s14">2</span><span class="s15"> </span>In particular, Kirstens Viewer [Cinquetti (AV) 2011] had, as one of its explicit goals, the extension and integration of graphics techniques, such as realistic shadows, depth of field, and other functionalities, that take better advantage of the flexibility afforded by programmable shaders.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">It was stated earlier that visual immersion has hewn very closely to real-time com- puter graphics until recently. This divergence corresponds to the aforementioned <i>type </i>of realism that is demanded by virtual world applications, which is not necessarily detail but more particularly a sense of transference, of transportation into a different environment or perceived reality. It can be stated that the visual fidelity afforded by improved processing power, display hardware, and graphics algorithms (particularly as enabled by programmable shaders) has crossed a certain threshold of sufficiency, such that improving the sense of visual immersion has shifted from pure detail to specific visual elements.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="48" height="0" alt="image" src="2480741.2480751_files/Image_009.png"/></span></p><p class="s16" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">2<span class="s17">This</span><span class="s2"> multiplicity of viewers for the same information space is analogous to the availability of multiple browser options for the Worldwide Web.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">One might consider then that stereoscopic vision may be a key component of visual immersion. This, to a degree, has been borne out by the recent surge in 3D films and particularly exemplified by the success of films with genuine three-dimensional data, such as <i>Avatar </i>and computer-animated work.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">A limiting factor of 3D viewing, however, is the need for special glasses in order to fully isolate the imagery seen by the left and right eyes. Although such glasses have become progressively less obtrusive and may eventually become completely unneces- sary, they still represent a hindrance of sorts, and represent a variant of the “come as you are” constraint taken from gesture-based interfaces, which states that ideally user interfaces must minimize the need for attachments or special constraints [Triesch and von der Malsburg 1998; Wachs et al. 2011].</p></li><li><p class="s19" style="padding-top: 7pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Sound. <span class="p">Like visual realism, the digital replication or generation of sound finds applications well beyond those of virtual worlds and can be argued to have had a greater impact on society as a whole than computer-generated visuals thanks to the CD player, MP3 and its successors, and home theater systems. Unlike visuals which are quite literally “in one’s face” when interacting with current virtual world environments, the role of audio is dual in nature, consisting of distinctly conscious and un- or subconscious sources of sound.</span></p><ol id="l9"><li><p class="s4" style="padding-top: 7pt;padding-left: 24pt;text-indent: -17pt;text-align: justify;">Conscious <span class="p">or </span>front-and-center <span class="p">audio in virtual worlds consists primarily of speech: Speaking and listening are our most natural and facile forms of verbal commu- nication, and the availability of speech is a significant factor in psychologically immersing and engaging ourselves in a virtual world because it engages us di- rectly with other inhabitants of that environment, perhaps more so than reading their (virtual) faces, posture, and movement.</span></p></li><li><p class="s4" style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">Ambient <span class="p">audio in virtual worlds consists of sound that we may not consciously process but whose presence or absence subtly influences the sense of immersion within that environment. This sense of sound immersion derives directly from the aural environment of the real world: we are at all times enveloped in sound, whether or not we are consciously listening to it. Such sound provides important positional and spatial cues, the perception of which contributes significantly to our sense of placement within a particular situation [Blauert 1996].</span></p></li></ol><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;text-align: right;">For the first conscious form of audio stimulus, we note that high-quality voice chat which reasonably captures the nuances of verbal communication is now generally available [Short et al. 1976; Lober et al. 2007; Wadley et al. 2009]. In this regard, the sense of hearing is somewhat ahead of sight in capturing avatar expression and inter- action. Future avenues in this area include voice masking and modulation technologies that not only capture the user’s spoken expressions but also customize the reproduced vocals seamlessly and effectively [Ueda et al. 2006; Xu et al. 2008; Mecwan et al. 2009]. Ambient audio, however, merits additional discussion. The accurate reproduction of individual sounds serves as a starting point: sound sample quality is limited today solely by storage and bandwidth availability as well as the dynamic range of audio output devices. However, as anyone who has listened to an extremely faithful lossless recording on a single speaker will attest, such one-dimensional accuracy is only the</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">beginning.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">In some respects, the immersiveness of sound can be seen as a broader-based pursuit than immersive visuals, with the ubiquity of sound systems on everything from mobile devices to game consoles to PCs to top-of-the-line studios and theaters. Over the years, stereo has given way to 2.1, 5.1, 7.1 systems and beyond, where the first value indi- cates the number of distinct directional channels and the second represents the num- ber of omnidirectional low-frequency channels. The very buzzword “surround sound”</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">communicates our desire to be completely enveloped within an audio environment— and we know it when we hear it. We just don’t hear it very often outside of the real world itself.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The correlation of <i>immersiveness </i>to <i>number of directional channels </i>is a straight- forward if ultimately simplistic approximation of how we perceive sound. Sound from the environment reaches our two ears at subtly different levels and frequencies, dif- ferences which the human brain can seamlessly merge and parse into strikingly com- plete positional information [Blauert 1996]. Multichannel sound attempts to replicate these variations by literally projecting different levels of audio from different locations. However, genuine three-dimensional sound perception is more complicated than mere division into channels, as such perception is subject to highly individualized factors, such as the size and shape of the head, the shape of the ears, and years or decades of training and fine-tuning.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The study of <i>binaural sound</i>, as opposed to discrete multichannel reproduction, seeks to record and reproduce sound in a manner that is more faithful to the way we hear in three dimensions. Such precision is modeled as a <i>head-related transfer function </i>(HRTF) that specifies how the ear receives sound from a particular point in space. The HRTF captures the effect of the myriad variables involved in true spatial hearing—acoustic properties of the head, ears, etc.—and determines precisely what must reach the left and right ears of the listener. As a result, binaural audio also requires headphones to have the desired effect, much the same way that stereoscopic vision requires glasses. Both sensory approaches require the precise isolation and coordination of stimuli delivered to the organ involved, so that the intended perceptual <i>computation </i>occurs in the brain.</p><p class="s20" style="text-indent: 0pt;line-height: 10pt;text-align: left;">=</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The theory and technology of binaural audio has actually been around for a strikingly long time, with the first such recordings being made as far back as 1881 and occasional milestones and products being developed through the 20th century [Kall Binaural Audio 2010]. Not surprisingly, binaural audio synthesis, thanks to the mathematics of HRTFs, has also been studied and implemented [Brown and Duda 1998; Cobos et al. 2010; Action Reaction Labs 2010]. What is surprising is its lack of application in vir- tual world technology, with no known systems adopting binaural synthesis techniques to create a genuinely spatial sound field for its users. The implementation of binaural audio in virtual worlds thus represents a clear signpost for increased realism (and thus immersiveness) in this area.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: right;">Like 3D viewing, binaural audio requires clean isolation of left- and right-side signals in order to be perceived properly, with headphones serving as the aural analog of 3D glasses. Unlike 3D viewing, individual physical differences affect the perceived sound field as well, such that perfect binaural listening cannot really be achieved unless recording conditions (or synthesis parameters) accurately model the listener’s anatomy and environment. Algorithmic tuning within the spatial audio environment in order to allow each listener to specifically match his/her hearing situation would be ideal. Much like we train voice recognition or use an equalizer in audio for best effect to the listener, binaural settings and tuning would allow the listener to adjust to their specific circumstances and gain the most from the experience. It can be argued, however, that the increased spatial fidelity that is achieved even by non-ideal binaural sound would still represent appreciable progress in the audio realism of a virtual world application. Binaural sound synthesis algorithms take as input a prerecorded sound sample, positional information, and one or more HRTFs. Multiplexing these samples may then produce the overall spatial audio environment for a virtual world. However, in much the same way that image textures of static resolution and variety quickly reveal their limitations as tiles and blocks, so does a collection of mixed-and-matched samples</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">eventually exhibit a degree of repetitiveness, even when presented spatially.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 4pt;padding-left: 94pt;text-indent: 0pt;text-align: left;">Table IV. Conceptual Equivalents in Visual and Audio Realism</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:31.453pt" cellspacing="0"><tr style="height:11pt"><td style="width:89pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:97pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s13" style="padding-left: 4pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">Sight</p></td><td style="width:156pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s13" style="padding-left: 5pt;padding-right: 5pt;text-indent: 0pt;line-height: 9pt;text-align: center;">Sound</p></td></tr><tr style="height:11pt"><td style="width:89pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s13" style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Static representation</p></td><td style="width:97pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s11" style="padding-left: 4pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">textures, fixed function</p></td><td style="width:156pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s11" style="padding-left: 5pt;padding-right: 5pt;text-indent: 0pt;line-height: 9pt;text-align: center;">prerecorded samples, discrete channels</p></td></tr><tr style="height:11pt"><td style="width:89pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s13" style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Procedural approach</p></td><td style="width:97pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s11" style="padding-left: 4pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">programmable shaders</p></td><td style="width:156pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s11" style="padding-left: 5pt;padding-right: 5pt;text-indent: 0pt;line-height: 9pt;text-align: center;">dynamic sound synthesis</p></td></tr><tr style="height:11pt"><td style="width:89pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s13" style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Spatial presentation</p></td><td style="width:97pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s11" style="padding-left: 4pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">stereoscopic vision</p></td><td style="width:156pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s11" style="padding-left: 5pt;padding-right: 5pt;text-indent: 0pt;line-height: 9pt;text-align: center;">binaural audio</p></td></tr><tr style="height:11pt"><td style="width:89pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s13" style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Spatial equipment</p></td><td style="width:97pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s11" style="padding-left: 4pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">goggles/glasses</p></td><td style="width:156pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s11" style="padding-left: 5pt;padding-right: 5pt;text-indent: 0pt;line-height: 9pt;text-align: center;">headphones</p></td></tr><tr style="height:11pt"><td style="width:89pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s13" style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Avatar expression</p></td><td style="width:97pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt;border-right-style:solid;border-right-width:1pt"><p class="s11" style="padding-left: 4pt;padding-right: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">face, gesture</p></td><td style="width:156pt;border-top-style:solid;border-top-width:1pt;border-left-style:solid;border-left-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s11" style="padding-left: 5pt;padding-right: 5pt;text-indent: 0pt;line-height: 9pt;text-align: center;">voice chat, masking</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">An additional layer of audio realism can be achieved through synthesis of the actual samples based on the characteristics of the objects generating the sound. Wind blowing through a tunnel will sound different from wind blowing through a field; a footfall should vary based on the footwear and the ground being traversed. Though subtle such audio cues influence our perception of an environment’s realism and thus our sense of immersion within it. When placed in the context of an open-ended virtual world, does this obligate the system to load up a vast library of prerecorded samples? In the same way that modern visuals cannot rely solely on predrawn textures for detail, one can say that the ambient sound environment should also explore the dynamic synthesis of sound effects—a computational foley artist, so to speak [van den Doel et al. 2001].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Work on precisely this type of dynamic sound synthesis, based on factors ranging from rigid body interactions, such as collisions, fractures, scratching, or sliding [O’Brien et al. 2002; Bonneel et al. 2008; Chadwick et al. 2009; Zheng and James 2010], to adjustments based on the rendered texture of an object [Schmidt and Dionisio 2008], exists in the literature. Synthesis of fluid sounds is also of particular interest, as bodies of water and other liquids are sometimes prominent components of a virtual environment [Zheng and James 2009; Moss et al. 2010]. These techniques have not, however, seen much implementation in the virtual world milieu.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Imagine the full effect of a virtual world audio environment where every subtle interaction of avatars and objects produces a reasonably unique and believable sound, which is then rendered, through real-time sound propagation algorithms and binaural synthesis, in a manner that accurately reflects its spatial relationship with the user’s avatar. This research direction for realistic sound within the Metaverse has begun to produce results and constitutes a desirable endpoint for truly immersive audio [Taylor et al. 2009].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Table IV seeks to summarize the realism discussion thus far, highlighting technolo- gies and approaches that are analogous to each other across these senses.</p></li><li><p class="s19" style="padding-top: 7pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Touch. <span class="p">After sight and sound, the sense of touch gets most of the remaining attention in terms of virtual world realism. It turns out that in virtual worlds the very notion of touch is more open to interpretation than sight and sound.</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The classic and perhaps most literal notion of touch in virtual environments is haptic or force feedback. The idea behind haptic feedback is to convert virtual contacts into physical ones. Force feedback is a specific, somewhat simpler form of haptic feedback that pertains specifically to having physical devices push against or resist the user’s body, primarily his or her hands or arms. Tactile feedback has been differentiated from haptic/force feedback as involving only the skin, as opposed to the muscles, joints, and entire body parts [Subramanian et al. 2005].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Haptics are a tale of two technologies: on the one hand, a very simple form of haptic feedback is widely available in game consoles and controllers in the form of vibrations timed to specific events within a game or virtual environment. In addition, user input delivered by movement and gestures, which by nature produces a sort of <i>collateral </i>haptic feedback, has also become fairly common, available in two forms: hand-in-hand with the vibrating controls of the same consoles and systems, and alongside mobile and</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">ultramobile devices that are equipped with gyroscopes and accelerometers. Microsoft’s Project Natal, which eventually became the Kinect product, took movement interfaces to a new phase with the first mainstream <i>come-as-you-are </i>motion system in which users no longer require a handheld or attached controller in order to convey movement.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Although this form of haptic feedback has become fairly commonplace, in many respects, such sensations remain limited, synthetic, and ultimately not very immer- sive. Feeling vibrations when colliding with objects or firing weapons, for example, constitutes a form of sensory mismatch and is ultimately interpreted as an indirect cue—hardly more realistic than say a system beep or an icon on the screen. Move- ment interfaces, although providing some degree of inertial or muscular sensation, ultimately only go halfway: if a sweeping hand gesture corresponds to a push in the virtual environment, the reaction to the user’s action remains relegated to visual or aural cues, or at most a vibration on a handheld controller.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Thus, targeted haptic feedback—bodily sensations that map more naturally to the virtual environment—remains an area of research and investigation. The work ranges from implementations for specific applications [Harders et al. 2007; Abate et al. 2009] to work that is more integrated with existing platforms, facilitated by open-source devel- opment [de Pascale et al. 2008]. Other work focuses on the haptic devices themselves— that is, the apparatus for delivering force feedback to the user [Folgheraiter et al. 2008; Bullion and Gurocak 2009; Withana et al. 2010]. Finally, the very methodology of cap- turing, modeling, and ultimately reproducing touch-related object properties has been proposed as <i>haptography </i>[Kuchenbecker 2008].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Any discussion of haptic or tactile feedback in virtual worlds would be incomplete without considering touch stimuli of a sexual nature. Sexually-motivated applications have been an open secret of technology adoption, and virtual worlds are no exception [Gilbert et al. 2011b]. So-called <i>teledildonic </i>devices, such as the Sinulator and the Fleshlight, seek to convey tactile feedback to the genital area with the goal of promoting the realism of virtual world sexual encounters [Lynn 2004]. Unfortunately, empirical studies of their effectiveness and overall role in virtual environments, if any, remain unpublished or unavailable.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Less sexual but potentially just as intimate are avatar interactions, such as hugs, cuddles, and tickles. Explorations in this direction have been made as well, in an area termed <i>mediated social touch </i>[Haans and IJsselsteijn 2006]. Such interactions require haptic garments or jackets that provide force or tactile feedback beyond the user’s hands [Tsetserukou 2010; Rahman et al. 2010]. A suite of such devices has even been integrated with Linden Lab’s open-source Second Life viewer to facilitate multimodal, affective, nonverbal communication among avatars in that virtual world [Tsetserukou et al. 2010]. Although such technologies still require additional equipment that may limit widespread adoption and use, they do reflect the aforementioned focus on en- hanced avatar interaction as a particular component of virtual world realism and immersion.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Despite the work in this area, the very presence of a physical device may ultimately dilute users’ sense of immersion (another manifestation of the come-as-you-are prin- ciple). Even with such attachments, no single device can yet convey the full spectrum of force and tactile stimuli that a real-world environment conveys. As a result, some studies have shown that user acceptance of such devices remains somewhat lukewarm or indifferent [Krol et al. 2009]. For networked virtual worlds in particular, technical issues such as latency detract from the effectiveness of haptic feedback [Jay et al. 2007]. The work continues however, as other studies have noted that haptic feedback, once it reaches sufficient fidelity to real-life interactions, does indeed increase one’s feeling of social presence [Sallna¨ s 2010].</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">The physical constraints of “true” haptic feedback,  particularly  as imposed by the need for physical attachments to the user, has triggered some interesting alternative paths. Pseudo-haptic feedback seeks to convey the perception of haptics by exploiting connections between visual and haptic stimuli [Le´cuyer 2009]. Further, the responsive- ness of objects to an avatar’s proximity and collisions conveys a sense of involvement and  interaction  that  may  stand  in  for  actual  haptic  or  force  feedback.  Such  prox- emic interactions are in fact sufficiently compelling that research in this area is being conducted even for real-world systems [Greenberg et al. 2011]. Interestingly, virtual proxemic interfaces have an advantage of sorts over their real-world counterparts: in a virtual environment, sensing the state of the world is a non-issue, because the envi- ronment itself manages that state. Having such an accurate reading of virtual objects’ and avatars’ locations, velocities, materials, and shapes can facilitate very rich and de- tailed proxemic behavior. Such behavior can in turn promote a sense of touch-related immersion without having to deliver actual force or touch stimuli at all.</p></li><li><p class="s19" style="padding-top: 6pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Other Senses and Stimuli. <span class="p">Smell and taste, as well as other perceived stimuli such as balance, acceleration, temperature, kinaesthesia, direction, time, and others, have not generally been as well-served by virtual world technologies as sight, sound, and touch. Of these, balance and acceleration have perhaps seen the most attention through motion and flight simulators. Such simulators take “immersion” literally, that is, complete physical enclosure—ultimately not practical for all but the most devout (and wealthy!) of virtual world users.</span></p></li><li><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><span class="s19">Gestures and Expressions. </span>It can be argued that the presence of and interactions with fellow human beings, as mediated through virtual world avatars, contributes strongly to the immersive experience that is deemed central to virtual world realism. As such, the more natural and expressive an avatar seems to be, the greater the per- ceived reality of the virtual environment. Traditional rendered detail for avatars has given way to affective cues like poses, incidental sounds, and facial expressions. Such cues range from the subtle, such as blinking, to the integrative, such as displaying mouth movement while audio chat is taking place. Anything that can make the avatar look more “alive” is worth exploring, as the appearance of life (or liveliness) translates to the sense of immersion [Paleari and Lisetti 2006; Martin et al. 2007; Geddes 2010]. Blascovich and Bailenson use the term <i>human social agency </i>to denote this character- istic. The primary variables of such agency have been stratified as movement realism (gestures, expressions, postures, etc.), anthropometric realism (recognizable human body parts), and photographic realism (closeness to actual human appearance), with the first two variables carrying greater weight than the third [Blascovich and Bailenson 2011].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Based on this premise, emerging work on the real-time rendering of human beings is anticipated as the next great sensory surge for virtual worlds. Such work includes increasingly realistic display and animation of the human form, not only of the body itself but of clothes and other attachments [Stoll et al. 2010; Lee et al. 2010]. In this effort, the face may be a particular area of focus—perhaps the new window to the “virtual soul,” at least until the rendering of eyes alone can capture the richness and depth of human emotion [Chun et al. 2007; Jimenez et al. 2010].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Alongside the output of improved visuals, audio, and touch is the requirement for the nonintrusive input of the data that inform such sensory stimuli, particularly with re- spect to avatar interaction and communication. As mentioned, many sensory technolo- gies rely on additional equipment beyond standard computing devices, ranging from the relatively common (headphones) to the rare or inconvenient (haptic garments)— and these are just for sensory output. Input devices are currently even more limited (keyboard, mouse) or somewhat untapped (camera), with only the microphone (in the</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">audio category) being close to fulfilling the multiple constraints that an input device be highly available and unobtrusive while delivering relative fidelity and integration into a virtual world environment.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Multiple research opportunities thus exist for improving the sensory input that can be used by virtual worlds, especially the kinds of input that enhance interaction among avatars, which, as we have proposed, is a crucial contributor to a system’s immersive- ness and thus realism. On the visual side, capturing the actual facial expression of the user at a given moment [Chandrasiri et al. 2004; Chun et al. 2007; Bradley et al. 2010] serves as an ideal complement to aforementioned facial rendering algorithms [Jimenez et al. 2010] and effectively removes artificial indirection or mediation via keyboard or mouse for facial expressions between real and virtual environments. Some work cov- ers the whole cycle, again indicative of the synergy between capture and presentation [Sung and Kim 2009].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Some input approaches are multimodal, integrating input for one sense with output for another. For example, head tracking, motion detection, or full-body video capture can produce more natural avatar animations [Kawato and Ohya 2000; Newman et al. 2000; Kim et al. 2010] and more immersive or intuitive user interfaces [Francone and Nigay 2011; Rankine et al. 2011]. Such cues can also be synchronized with supporting stimuli, such as sounds (speech, breathing), environmental visuals (depth of field, subjective lighting), and other aspects of the virtual environment [Arangarasan and Phillips Jr. 2002; Latoschik 2005]. Integrating technologies so that the whole becomes greater than the sum of its parts is a theme that is particularly compelling for virtual worlds—a supposition that not only makes intuitive sense but has also seen support from published studies [Biocca et al. 2001; Naumann et al. 2010] and integrative presentations on the subject at conferences, particularly at SIGGRAPH [Fisher et al. 2004; Otaduy et al. 2009].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The need to have multiple devices, wearables, and other equipment for deliver- ing these stimuli diverges from the come-as-you-are principle and poses a barrier to widespread use. This has motivated the pursuit of neural interfaces which seek to de- liver the full suite of stimuli directly to and from the brain [Charles 1999; Gosselin and Sawan 2010; Cozzi et al. 2005]. Although this technology is extremely nascent and pre- liminary, it may represent the best potential combination of high-fidelity, high-detail delivery of sensory stimuli with low device intrusiveness.</p></li><li><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><span class="s19">The Uncanny Valley Revisited. </span>Our discussion of realism in virtual worlds con- cludes with <i>the uncanny valley</i>, a term coined by Masahiro Mori to denote an apparent discontinuity between the additive likeness of a virtual character or robot to a human being and an actual human being’s reaction to that likeness. As seen in Figure 1<span class="s14">3</span>, the recognized humanity of a character or robot appears to increase steadily up to a certain point, at which human reactions turn negative or eerie at best, before “full” human like- ness is attained [Mori 1970]. The phenomenon, observable in both computer-generated characters and humanoid physical devices, suggests that recognizing “full humanity” may involve a diverse collection of factors, including sensory subtleties, emotional or empathetic tendencies, diverse behaviors, and perceptual mechanisms for processing faces [Seyama and Nagayama 2007, 2009; Misselhorn 2009; Tinwell et al. 2011].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Although the term is frequently seen in the context of video games, computer ani- mation, and robotics, the uncanny valley takes on a particular significance for virtual world realism, because this form of realism involves psychological immersion, driven primarily by how we perceive and interact with avatars. As we become more proficient</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="48" height="0" alt="image" src="2480741.2480751_files/Image_010.png"/></span></p><p class="s16" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">3<span class="s17">Based</span><span class="s2"> on a translation by Karl F. MacDorman and Takashi Minato, manually rendered and uploaded to Wikimedia Commons [Smurrayinchester (AV) and Voidvector (AV) 2008].</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="4" alt="image" src="2480741.2480751_files/Image_011.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="4" height="1" alt="image" src="2480741.2480751_files/Image_012.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="234" height="212" alt="image" src="2480741.2480751_files/Image_013.png"/></span></p><h4 style="text-indent: 0pt;text-align: left;">uncanny valley</h4><p style="text-indent: 0pt;text-align: left;"/><p class="s21" style="padding-left: 5pt;text-indent: -5pt;line-height: 124%;text-align: left;">moving  <u>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </u> still</p><p style="text-indent: 0pt;text-align: left;"/><p class="s21" style="text-indent: 0pt;text-align: left;">bunraku puppet</p><p style="text-indent: 0pt;text-align: left;"/><p class="s21" style="text-indent: 0pt;text-align: left;">humanoid robot</p><p style="text-indent: 0pt;text-align: left;"/><p class="s21" style="text-indent: 0pt;text-align: left;">stuffed animal</p><p style="text-indent: 0pt;text-align: left;"/><p class="s21" style="text-indent: 0pt;text-align: left;">industrial robot</p><p style="text-indent: 0pt;text-align: left;"/><h4 style="text-indent: 0pt;text-align: left;">human likeness</h4><p style="text-indent: 0pt;text-align: left;"/><p class="s23" style="text-indent: 0pt;text-align: left;">50%</p><p style="text-indent: 0pt;text-align: left;"/><p class="s21" style="text-indent: 0pt;text-align: left;">corpse</p><p style="text-indent: 0pt;text-align: left;"/><p class="s23" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">100%</p><p class="s21" style="padding-top: 4pt;text-indent: 0pt;text-align: left;">prosthetic hand</p><p style="text-indent: 0pt;text-align: left;"/><p class="s21" style="text-indent: 0pt;text-align: left;">zombie <u>&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;</u></p><p style="text-indent: 0pt;text-align: left;"/><p class="s10" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">Fig. 1<span class="s2">. Masahiro Mori’s uncanny valley. (credit: http://commons.wikimedia.org/wiki/File:Main_ Uncanny_Valley.svg.)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">at rendering certain types of realism, we then have to try to understand which compo- nents of that realism have the greatest psychological impact, both individually and in concert with each other (the so-called multimodal approaches mentioned previously). Factors such as static facial detail versus animation, as well as body shape and move- ment have been studied for their ability to convey emotional content or to elicit trust [Hertzmann et al. 2009; McDonnell et al. 2009; McDonnell and Breidt 2010]. Some studies suggest that the uncanny valley response may also vary based on factors be- yond the appearance or behavior of the human facsimile, such as by the gender, age, or even technological sophistication of the human observers [Ho et al. 2008; Tinwell and Grimshaw 2009].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">As progress has been made in studying the phenomenon itself, so also has progress been made in crossing or transcending it. The computer graphics advances presented in this section have contributed to this progress in general, with research in facial mod- eling and rendering leading the way in particular. Proprietary algorithms developed by Kevin Walker and commercialized by Image Metrics use computer vision and image analysis of prerecorded video from human performers to generate virtual characters that are recognizably lifelike and can credibly claim to cross the uncanny valley [Im- age Metrics 2011]. The company’s Faceware product has seen effective applications in a variety of computer games and animated films as well as research that specifically targets the uncanny valley phenomenon [Alexander et al. 2009]. These techniques are still best applied to pre-rendered animations, with dynamic real-time synthesis posing an ongoing challenge [Joly 2010]. Alternative approaches include the modeling of real facial muscular anatomy [Marcos et al. 2010] and rendering of the entire human form for applications, such as sports analysis or telepresence [Vignais et al. 2010; Aspin and Roberts 2011].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">For virtual world avatars to cross the uncanny valley, the challenge of effective visual rendering is compounded by (1) the need to achieve this rendering fully in real time and</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">(2) the use of real-time input (e.g., camera-captured facial expressions, synchronized voice chat, appropriate haptic stimuli). In these respects, virtual world environments pose the ultimate uncanny valley challenge, demanding bidirectional real-time capture, processing, and rendering of recognizably lifelike avatars. Such avatars in turn are instrumental in producing the most immersive psychologically compelling experience possible.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li><li><h2 style="padding-top: 5pt;padding-left: 23pt;text-indent: -18pt;text-align: left;">Ubiquity</h2><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">The notion of ubiquity in virtual worlds derives directly from the prime criterion that a fully-realized Metaverse must provide a milieu for human culture and interaction that, as with the physical world, is psychologically compelling for the user. The real world is ubiquitous in a number of ways. First, it is literally ubiquitous—we unavoidably dwell in, move around, and interact with it at all times, in all situations. Second, our presence within the real world is <i>ubiquitously manifest</i>—that is, our identity and persona are, under normal circumstances, universally recognizable, primarily through our physical embodiment (face, body, voice, fingerprints, retina) but augmented by a small universally-applicable set of artifacts, such as our signature, key documents (birth certificates, passports, licenses, etc.), and identifiers (social security numbers, bank accounts, credit cards, etc.). Our identity is further extended by what we produce and consume: books, music, or movies that we like; the food that we cook or eat; and memorabilia that we strongly associate with ourselves or our lives [Turkle 2007].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Although our perception of the real world may sometimes fluctuate—such as when we are asleep—the real world’s literal ubiquity persists regardless of what our senses say (and whether or not we are even alive). Similarly, although our identities may sometimes be doubted, inconclusive, forged, impersonated, or stolen, in the end, these situations are borne of errors, deceptions, and falsehoods: there always remains a real “me” with a core package of artifacts (physical, documentary, etc.) that authoritatively represent “me.”</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">To serve as a rich alternative venue for human activity and interaction, virtual worlds must support some analog of these two aspects of real-world ubiquity. If a virtual world does not retain some constant presence and availability, then it feels remote and far-removed and, thus, to some degree not as “real” as the physical world. If there are barriers, artificial impedances, or undue inconveniences involved with identifying ourselves and the information that we create or use within or across virtual worlds, this distances our sense of self from these worlds, and we lose a certain degree of investment or immersion within them. We thus divide the remainder of this section between these two aspects of ubiquity: ubiquitous availability and access, and ubiquitous persona and presence.</p><ol id="l10"><li><p style="padding-top: 10pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><span class="s19">Availability and Access of Virtual Worlds. </span>Ubiquitous availability of and access to vir- tual worlds can be viewed as a specific instance of <i>ubicomp </i>or <i>ubiquitous computing</i>, as first proposed by Mark Weiser [1991, 1993]. The general definition of ubiquitous computing is the massive pervasive presence of digital devices throughout our real- world surroundings. Ubiquitous computing applications include pervasive information capture, surveillance, interactive rooms, crowdsourcing, augmented reality, biometrics and health monitoring, and any other computer-supported activity that involves multi- ple, autonomous, and sometimes mobile loci of information input, processing, or output [Jeon et al. 2007].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The pervasive device aspect of ubicomp research—embedded systems, audio/video capture, ultramobile devices—can be of great service to the virtual worlds realm, es- pecially in terms of a world’s ubiquitous availability. Physical access to virtual worlds has predominantly been the realm of traditional computing devices: PCs and laptops ideally equipped with headsets, microphones, high-end graphics cards, and broadband connectivity. Visiting virtual worlds involves physically locating oneself at an appropri- ate station, whether it be a fixed desktop or a suitable setting for a laptop. Peripheral concerns, such as connecting a headset, finding a network connection, or ensuring an appropriate aural atmosphere (i.e., no excessive noise) frequently arise, and it is only after one is “settled in” that entry into the virtual environment may satisfactorily take place.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: right;">Although other features, such as realism and scalability (Section 3.4), may continue to be best served by traditional desktop and laptop stations, the current near-exclusivity of such devices as portals into virtual worlds hinders the perceived ubiquity of such worlds. When one is traveling away from one’s “usual” system or is in an otherwise “unwired” real-world situation, the inability to access or interact with a virtual world diminishes that world’s capacity to serve as an alternative milieu for cultural, social, and creative interaction—the very measure by which we differentiate virtual worlds from other 3D, networked, graphical, or social computer systems. Ubiquitous comput- ing lays the technological roadmap that leads away from this current desktop and laptop hegemony toward pervasive versatile availability and access to the Metaverse. The latest wave of mobile and tablet devices comprises one aspect of the ubiquitous computing vision that has recently hit mainstream status. Combined with existing alternative access points to virtual worlds, such as email and text-only chat protocols, these devices form the beginnings of ubiquitous virtual world access [Pocket Metaverse 2009]. Going forward, the increasing computational and audiovisual power of such devices may allow them to run viewers that approach current desktop/laptop viewers in fidelity and immersiveness. In addition, the new form factors and user interfaces enabled by such devices, with front-facing cameras, accelerometers, gyroscopes, and multitouch screens, should also facilitate new forms of interaction with virtual worlds beyond the keyboard-mouse-headset combination that is almost universally used today [Baillie et al. 2005; Francone and Nigay 2011]. Such technologies make the ubiquitous availability of virtual worlds more compelling and immersive in a manner that is</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">different from traditional desktop/laptop interaction.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">In parallel, modern Web applications and Web browsers, with their full programma- bility, standardized networking, 3D, and media interfaces, and zero-overhead instal- lation or access mechanisms, comprise a software-side enabler toward virtual world ubiquity. Modern Web architectures and browsers have already facilitated the ubiq- uity of e-Commerce, social networking, and other applications. Virtual worlds can also attain this degree of availability as Web standards, in conjunction with the platforms on which they run, begin to fulfill the requirements of a 3D virtual environment. The potential of such a platform has seen early demonstrations in systems like the Aves engine [Bakaus 2010b; 2010a]. Aves graphics were restricted to isometric 2D, but there is no reason to doubt that full 3D rendering using WebGL will eventually arrive.<span class="s14">4</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">It should be noted that not all <i>browser-based </i>virtual environments are entirely browser-native, requiring third-party plugins to deliver an immersive experience. Such implementations only partially fulfill the ubiquity potential of a browser-based virtual world viewer and may require significant porting or redesign to become browser-based in the true sense of the word.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">In the end, the broad permeation of real-world portals into a virtual world, along the lines envisioned by ubiquitous computing, will elevate a virtual world’s ability to be an alternative and equally rich environment to the physical world: a true parallel existence to and from which we can shuttle freely with a minimum of technological or situational hindrances. The current candidate platform for this vision of ubiquitous availability and access is a modern Web application built on open-standard browser- native technologies, such as HTML5, Ajax, and WebGL, which, through its reliance solely on a Web browser without third-party plugins, can then run on the full spectrum of devices with standards-compliant browsers.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="48" height="0" alt="image" src="2480741.2480751_files/Image_014.png"/></span></p><p class="s16" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">4<span class="s17">The</span><span class="s2"> Aves project was acquired by Zynga and has been brought completely inhouse. However, the prospect of an open or freely available engine of this type remains likely and bright.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p class="s19" style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Manifest Persona and Presence. <span class="p">As mentioned, in the real world, we have a unified presence, centered on our physical body but otherwise permeating other locations and aspects of life through artifacts or credentials that represent us. Bank accounts, credit cards, memberships in various organizations, and other affiliations carry our manifest persona, the sum total of which contributes to our overall presence in society and the world.</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">This aggregation of data pertaining to our identity has begun to take place within personal, social, medical, financial, and other information spaces. The strength of this data aggregation varies, ranging from seamless integration (e.g., between two systems that use the same electronic credentials) to total separation within the digital space, only integrated outside via user input or interaction. If current trends continue, these informational fragments of identity should tend toward consolidation and interoper- ability, producing a distributed ubiquitous electronic presence [Gilbert et al. 2011a; Gilbert and Forney 2013]. This form of ubiquity must also be modeled by virtual en- vironments as a condition for their being a sufficient alternative existence to the real world.</p><p style="padding-left: 5pt;text-indent: 9pt;line-height: 94%;text-align: justify;">As information systems have evolved from passive recorders of transactions, with the types of transactions themselves evolving from financial activity to interpersonal communication, documents, and audiovisual media, and finally to social outlets, the flow of such information has also shifted from separated producers and consumers to so-called <i>prosumers </i>that simultaneously create, view, and modify content.<span class="s14">5</span><span class="s15"> </span>A persona in this context is the virtual aggregate of a person’s online presence, in terms of the digital artifacts that he or she creates, views, or modifies.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The prosumer culture has entrenched itself well in blogs, mashups, and audiovisual and social media. Their close association to the individuals that produce and consume this content has provided greater motivation toward consolidation of persona than traditional information <i>wells </i>such as finance and commerce. Common information from social networks can be utilized across systems through published protocols and services resulting in increasing unification and consolidation of one’s individuality [Facebook 2011; Twitter 2011; Yahoo! Inc. 2011; OAuth Community 2011; OpenID Foundation 2011]. This interconnectivity and transfer of persona when and where it is required by the user has yet to see adoption or implementation within virtual worlds. Virtual worlds are currently balkanized information silos with the credentials required being just as disjoint and uncoordinated.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Shared credentials and the bidirectional flow of current information types, such as writings, images, audio, and video, across virtual worlds represent only the initial phases of bringing a ubiquitous unified persona and presence into this domain. The actual content that can be produced/consumed within virtual environments expands as well, now covering 3D models encapsulating interactive and physics-based behaviors as well as <i>machinima </i>presentations delivering self-contained episodes of virtual world activity [Morris et al. 2005]. Where interoperability (covered in the next section) con- cerns itself with the development of sufficiently extensible standards for conveying or transferring such digital assets, ubiquity concerns itself with consistently identifying and associating such assets with the personas that produce or consume them. Virtual worlds must provide this aggregation of virtual artifacts under a unified omnipresent electronic “self ” in order to serve as a viable real-world alternative existence. Digital assets which comprise a virtual self must also be seamlessly available from all points of virtual access, as illustrated in Figure 2.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="48" height="0" alt="image" src="2480741.2480751_files/Image_015.png"/></span></p><p class="s16" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">5<span class="s17">The</span><span class="s2"> term “prosumer” here combines the terms “producer” and “consumer,” and not “professional consumer,” as the term is used in other domains.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 113pt;text-indent: 0pt;text-align: left;"><span><img width="241" height="241" alt="image" src="2480741.2480751_files/Image_016.jpg"/></span></p><p class="s10" style="padding-top: 7pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Fig. 2<span class="s2">. Interactions (or lack of) among social media networks and virtual environments.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">To date, no system facilitates ease of migration for the virtual environment persona across virtual worlds, unlike the interchange protocols in place for social media outlets and a host of other information hosting and sharing services. There are some efforts to create unified standards for virtual environments [IEEE VW Standard Working Group 2011a; COLLADA Community 2011; Arnaud and Barnes 2006; Smart et al. 2007]. At present, these efforts have yet to bear long-term fruit.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">In order to truly offer a ubiquitous persona in virtual environments, the individual must be able to seamlessly and transparently cross virtual borders while remaining connected to existing legally-accessible information assets. A solution for this require- ment may involve not only industry consensus but also some technical innovation that can connect these systems and store assets securely for transfer while remaining independent of any single central influence.</p></li></ol></li><li><h2 style="padding-top: 8pt;padding-left: 23pt;text-indent: -18pt;text-align: left;">Interoperability</h2><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">In terms of functionality alone, interoperability in the virtual world context is little different from the general notion of interoperability: it is the ability of distinct sys- tems or platforms to exchange information or interact with each other seamlessly and, when possible, transparently. Interoperability also implies some type of consensus or convention which then become standards when formalized.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">As specifically applied to virtual worlds, interoperability might be viewed as merely the enabling technology required for ubiquity, as described in the previous section. Although this is not inaccurate, interoperability remains a key feature of virtual worlds in its own right, because it is interoperability that puts the capital M in the Metaverse: just as the singular capitalized Internet is borne of layered standards which allow disparate heterogeneous networks and subnetworks to communicate with each other transparently, so too will a singular capitalized Metaverse only emerge if corresponding standards also allow disparate heterogeneous virtual worlds to seamlessly exchange or transport objects, behaviors, and avatars.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">This desired behavior is analogous to travel and transport in the real world. As our bodies move between physical locations, our identity seamlessly transfers from point to point with no interruption of experience. Our possessions can be sent from place to place, and under normal circumstances, they do not substantially change in doing so. Thus, real-world travel has a continuity in which we and our things remain largely</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">intact in transit. We take this continuity for granted in the real world where it is indeed a matter of course. With virtual worlds, however, this effect ranges from highly disruptive to completely nonexistent.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The importance of having a single Metaverse is connected directly to the long-term endpoint of having virtual worlds offer a milieu for human sociocultural interaction that, like the physical world, is psychologically rich and compelling. Such integra- tion immediately makes all compatible virtual world implementations, regardless of lineage, parts of a greater whole. With interoperability, especially in terms of a transfer- able avatar, users can finally have full access to any environment without the disruption of changing login credentials or losing one’s chain of cross-cutting digital assets.</p><ol id="l11"><li><p class="s19" style="padding-top: 8pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Existing Standards. <span class="p">The first (relatively) widely-adopted standard relating to vir- tual worlds was VRML (Virtual Reality Modeling Language). Originally envisioned as a 3D-scene analog to the then-recently-emergent HTML document standard, VRML captured an individual 3D space as a single “world” file that can be downloaded and displayed with any VRML-capable browser. VRML, like HTML, utilized URLs to fa- cilitate navigation from one world to another, thus enabling an early form of virtual world interoperability.</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">VRML has since been succeeded by X3D. X3D expands on the graphics capabilities of VRML while also adjusting its syntax to align it with XML. Both formats reached formal ISO recognition as standards but neither has gained truly mainstream adop- tion. Barriers to this adoption include the lack of a satisfactory widely-available client for effectively displaying this content, competition from proprietary efforts, and most importantly, a lack of critical mass with regard to actual virtual world content and richness. Efforts are currently under way for making X3D a first-class citizen of the HTML5 specification in the same way that other XML dialects, like SVG and MathML, have been integrated. Such inclusion may solve the issue of client availability as Web browser developers would have to support X3D if they are to claim full HTML5 compli- ance, but overcoming the other barriers—especially that of insufficient content—would require additional effort and perhaps some degree of advocacy.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Technologically distinct and more recent than VRML and X3D is COLLADA [COLLADA Community 2011; Arnaud and Barnes 2006]. COLLADA has a different design goal: it is not as much a complete virtual world standard as it is an interchange format. Widespread COLLADA adoption would facilitate easier exchange of objects and behaviors from one virtual world system to another; in fact, COLLADA’s scope is not limited to virtual worlds because it can be used as a general-purpose 3D object interchange mechanism.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">On the opposite side of the standards aisle is the <i>de facto </i>standard established by Linden Lab which stems purely from the relative success of Second Life. Unlike X3D and VRML before it, Second Life crossed an adoption threshold that spawned a fairly viable ecology of alternative viewers and servers. Beyond this widespread use, however, Second Life user credentials, protocols, and data formats fall short. They remain proprietary with an array of restrictions on how and where they can be used. For instance, only viewers (clients) have official support from Linden Lab; alternative server implementations, such as OpenSimulator, are actually reverse-engineered and not directly sanctioned.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">As with other areas with interoperability concerns, virtual world interoperability is not an entirely technical challenge. Many of its current players actually benefit from a lack of interoperability in the short term and thus understandably resist it or are apathetic to it. Similar to other areas (such as the emergence of the Worldwide Web above proprietary online information services), a potential push toward interoperability requires multilateral contributions and momentum from standards groups, commercial developers, and the open-source community.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p class="s19" style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Virtual World Interoperability Layers. <span class="p">The promise of virtual world interoperability as envisioned in this article—the kind that facilitates a single Metaverse and not a disjointed partitioned scattering of incompatible environments—is currently a work in progress. Ideally, such a standard would have the rigor and ratification achieved by VRML and X3D while also attaining a critical mass of active users and content. In addition this standard should actually be a family of standards, each concerned with a different layer of virtual world technology.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l12"><li><p style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">A <i>model standard </i>should sufficiently capture the properties, geometry, assets, and perhaps behavior of virtual world environments. VRML, X3D, and COLLADA are predominantly model standards. Note that a model standard’s main purpose is to facilitate object interchange across different virtual world systems. The systems themselves may represent objects using a private or proprietary mechanism as long as they seamlessly read from and write to the nonproprietary open standard. Modeling is the most obvious area that needs standardization, but this alone will not facilitate the capital-M Metaverse.</p></li><li><p style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">A <i>protocol standard </i>defines the interactive and transactional contract between a virtual world client or viewer and a virtual world server. This layer enables a mix- and-match situation with multiple choices for virtual world clients and servers, none of which would be “wrong” from the perspective of compatibility. As men- tioned, Second Life has established a de facto protocol standard solely through its widespread use. Full standardization, at this writing, does not appear to be part of this protocol’s road map.</p><p style="padding-left: 24pt;text-indent: 9pt;text-align: justify;">Open Cobalt currently represents the most mature effort other than Second Life that can potentially fulfill the role of a virtual world protocol standard [Open Cobalt Project 2011]. As an open-source platform under the flexible MIT free software license, Open Cobalt certainly offers fewer limits or restrictions to widespread adoption. Like VRML and X3D, however, the protocol will need connections to diverse and compelling content in order to gain critical mass.</p></li><li><p style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">A <i>locator standard </i>identifies places or landmarks across virtual worlds. Techno- logically, this may be the easiest standard to establish because it already exists: uniform resource locators (URLs), a subset of the uniform resource identifier (URI) standard [Berners-Lee et al. 2005], are completely adaptable for virtual world land- marks and have in fact been used by Linden Lab for Second Life locations (Second Life URLs or SLURLs). The missing piece is universal consensus and adoption of a virtual world URL scheme (VWURLs, so to speak, or MVURLs if the preferred phrase is <i>Metaverse URL</i>) so that, regardless of the virtual world system or viewer, the same scheme can be recognized and used to move easily from virtual place to virtual place.</p></li><li><p style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">An <i>identity standard </i>defines a unified set of user credentials and capabilities that can cross virtual world platform boundaries: This would be the virtual world equivalent of efforts such as OpenID and the Facebook Connect [OpenID Founda- tion 2011; Facebook 2011]. The importance of an identity standard was covered in Section 3.2.2.</p></li><li><p style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">A <i>currency standard </i>quantifies the value of virtual objects and creations, facili- tating their trade and exchange. Linden Dollars within Second Life represent the dominant virtual world currency, but it can be used only within Second Life and is meaningful only within that system (until exchanged for a real-world currency). Bitcoin is not controlled by any particular authority, with a number of mechanisms in place for tracking or validating its transactions while retaining a peer-to-peer nature [Nakamoto 2009; Piotrowski and Terzis 2011]. A standard such as this may form the basis for interoperable Metaverse currency.</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">As with Internet standards, this layering is defined only to separate the distinct virtual world concerns, to help focus on each area’s particular challenges and require- ments. Ultimately, standards at every layer will need to emerge, with the combined set forming an overall framework around which virtual worlds can connect and coa- lesce into a single unified Metaverse capable of growing and evolving in a parallel, decentralized, and organic manner.</p></li></ol></li><li><h2 style="padding-top: 10pt;padding-left: 23pt;text-indent: -18pt;text-align: left;">Scalability</h2><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">As with the features discussed in the previous sections, virtual worlds have scalability concerns that are similar to those that exist with other systems and technologies, while also having some distinct and unique issues from the virtual world perspective. Based on this article’s prime criterion that a fully-realized Metaverse must provide a milieu for human culture and interaction, scalability may thus be the most challenging virtual world feature of all, as the physical world is of enormous and potentially infinite scale on many levels and dimensions. Three dimensions of virtual world scalability have been identified in the literature [Liu et al. 2010].</p><ol id="l13"><li><p class="s4" style="padding-top: 7pt;padding-left: 24pt;text-indent: -17pt;text-align: justify;">Concurrent Users/Avatars: <span class="p">The number of users interacting with each other at a given moment.</span></p></li><li><p class="s4" style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">Scene Complexity: <span class="p">The number of objects in a particular locality and their level of detail or complexity in terms of both behavior and appearance.</span></p></li><li><p class="s4" style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">User/Avatar Interaction: <span class="p">The type, scope, and range of interactions that are possi- ble among concurrent users (e.g., intimate conversations within a small space vs. large-area crowd-scale activities such as “the wave”).</span></p></li></ol><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">In many respects, the virtual world scalability problem parallels the computer graph- ics rendering problem: what we see in the real world is the constantly updating result of multitudes of interactions among photons and materials governed by the laws of physics—something that computers can only approximate and never absolutely repli- cate. Virtual worlds add further dimensions to these interactions, with the human social factor playing a key role (as previously expressed in the first and third dimen- sions). Thus it is no surprise that most of the scientific problems listed by Zhao [2011] focus on theoretical limits to virtual world modeling and computation; because in the end, what else is a virtual world but an attempt to simulate the real world in its entirety, from its physical makeup all the way to the activities of its inhabitants?</p><ol id="l14"><li><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><span class="s19">Traditional Centralized Architectures. </span>Historically, virtual world architectures can be viewed as the progeny of graphics-intensive 3D games and simulations, and thus they share the strengths and weaknesses of both [Waldo 2008]. Graphics-intensive 3D games focus on scene complexity and detail, particularly in terms of object rendering or appearance, and strongly emphasize real-time performance. Simulations focus on in- teractions among numerous elements (or <i>actors</i>) with a focus on repeatability, accuracy, and precision.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Both 3D games and simulations, along with many other application categories at the time, were predominantly single-threaded and centralized when the first virtual world systems emerged. Processing followed a single serialized computational path, with some central entity serving as the locus of such computations, the authoritative repository of system state, or both. Virtual world implementations were no exception; as 3D-game-and-simulation hybrids, however, they consisted of two <i>centers</i>: clients or viewers that rendered the virtual world for its users and received the actions of its avatars (derived from 3D games) and servers that tracked the shared virtual world state, changing it as avatar and scripted directives were received from the clients connected to them (derived from simulations).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">This initial paradigm was in many respects unavoidable and quite natural, because it was the reigning approach for most applications at the time, including the 3D game and simulation technologies that converged to form the first virtual world systems. However, this initial paradigm also established a form of technological inertia, with succeeding virtual world iterations remaining conceptually centralized despite early evidence that such an architecture ultimately does not scale along any one of the aforementioned dimensions (concurrent avatars, scene complexity, avatar interactions) much less all three [Lanier 2010; Morningstar and Farmer 1991]. This “technology lock-in” resulted in successive attempts at virtual world scalability that addressed the symptoms of the problem and not its ultimate cause [Lanier 2010].</p></li><li><p class="s19" style="padding-top: 6pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Initial Distribution Strategies: Regions and Shards (Distribution by Geography). <span class="p">The role of clients/viewers as renderers of an individual user’s view of the virtual world was perhaps the most obvious initial boundary across which computational load can be clearly distributed: the same geometric, visual, and environmental information can be used by multiple clients to compute differing views of the same world in parallel. Outside of this separation, the maintenance and coordination of this geometric, visual, and environmental data—the shared state of the virtual world—then became the initial focus of scalability work.</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Initial approaches to distributing the maintenance of shared state fall within two broad categories: <i>regions </i>and <i>shards</i>. Both distribution strategies are based on the virtual world’s <i>geography</i>. In the region approach, the total expanse of a virtual world is divided into contiguous, frequently uniform chunks, with each chunk being given to a distinct computational unit or <i>sim</i>. This division is motivated by an assumption that activities which affect each other are most likely to take place within a relatively small area (i.e., spatial locality). Spatial locality implies that an individual avatar is primarily concerned with or affected by activities within a finite radius of that avatar. The <i>scaling </i>rationale thus sees the growth of a virtual world as the accretion of new regions, reflected by an increase in active sims. Individual avatars are then serviced by only one computational unit or host at a time because events of concern are thought to be restricted to within a small radius of that avatar.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The shard approach is also based on a virtual world’s geography, with portions of that geography being replicated rather than partitioned across multiple hosts or servers. Shards prioritize concurrent users over a single world state. Because parts of the virtual world are effectively copied across multiple servers, more users can seem to be located in a given area at a given time. Trade-offs for this concurrency include not being able to interact with every user in the same area (because users are separated over multiple servers). Most severely, as mentioned, the virtual world loses its single shared state. Thus shards are applicable primarily to transient virtual environments, such as massively multiplayer online games, and not to general virtual worlds that must maintain a single, persistent, and consistent state.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The region approach has been the de facto distribution strategy for many virtual world systems, including Second Life [Wilkes 2008]. However, as pointed out by Ian Wilkes and borne out by subsequent empirical studies [Gupta et al. 2009; Liu et al. 2010], partitioning by region fails on key virtual world use cases.</p><ol id="l15"><li><p class="s4" style="padding-top: 5pt;padding-left: 24pt;text-indent: -17pt;text-align: justify;">Number of concurrent users in a single region<span class="p">. Even the most powerful sims sat- urate at around 100 concurrent users (and thus a potential of 4,950 concurrent user-to-user interactions, not to mention interactions with objects).</span></p></li><li><p class="s4" style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">Varying complexity and detail in regions. <span class="p">The spatial basis for regions does not take into account the possibility that one region may be relatively barren and inactive while another may include tens of thousands of objects with multiple concurrent users.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p class="s4" style="padding-top: 5pt;padding-left: 24pt;text-indent: -17pt;text-align: justify;">Scope of user interaction. <span class="p">Virtual world users do not necessarily interact solely with users within the same region; services such as instant messaging and currency exchange may cross region boundaries.</span></p></li><li><p class="s4" style="padding-left: 24pt;text-indent: -17pt;text-align: justify;">Activity at region boundaries. <span class="p">Because region boundaries are actually transitions from one sim (host) to another, activities across such boundaries (travel, avatar interaction) incur additional latency due to data/state transfer.</span></p></li></ol><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">In the case of Second Life [Wilkes 2008], the third use case is addressed through the creation of separate services distinct from the grid of region sims. Such services include user/login management, voice chat, and raw data or asset storage. The separation of these subsystems did lighten the load on regions somewhat, but only for very specific types of load.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The fourth case is addressed by policy, through avoiding adjacent regions when possible. The first and second cases remain unresolved beyond the use of increasingly powerful individual servers and additional network bandwidth. OpenSimulator, as a reverse-engineered port of the Second Life architecture, thus also manifests the same strengths and weaknesses of that architecture [OpenSimulator Project 2011]. It has, however, also served as a basis for more advanced work on virtual world scalability [Liu et al. 2010].</p></li><li><p class="s19" style="padding-top: 7pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Distribution by Regions and Users. <span class="p">Future iterations of Second Life propose to sep- arate computation and information spaces into two distinct dimensions: separation of user/avatar state from region state and the establishment of multiple such user and region grids [Wilkes 2008]. The overall interaction space and computational load are thus divided by two linear factors: user/avatar versus region interactions and distinct clusters of users/avatars and regions. It is currently unknown whether this transition has been accomplished by Linden Lab.</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">This direction, particularly the accommodation of multiple grids of user and region servers and the ability to add, remove, or reallocate distinct computational units for handling user and world interactions, resembles a peer-to-peer (P2P) architecture in topology. However, unless interfaces for these units are cleanly abstracted and pub- lished and until new instances of user or region servers are permitted to start/stop on any Internet-connected host, this projected approach remains strictly within the boundaries of Linden Lab.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The Solipsis and OpenCobalt projects do not have such boundaries and so can be viewed as P2P architectures in terms of both technology and policy [Keller and Simon 2002; Frey et al. 2008; Open Cobalt Project 2011]. Both systems allow instances to dynamically augment the virtual world network’s geography (through <i>nodes </i>in Solip- sis and <i>spaces </i>in OpenCobalt) and user space (a user’s running peer serves as the origin for that user’s avatar within the virtual world). Solipsis uses neighborhood al- gorithms to locate and interact with other nodes, while OpenCobalt employs an LDAP registry to publish available spaces, with private spaces remaining accessible if a user knows typical network parameters, such as an IP address and port number. Interac- tions in Solipsis are scoped through an area of interest (AOI) approach (awareness radius), while OpenCobalt employs reliable replicated computation based on David Reed’s TeaTime protocol [Open Cobalt Project 2011; Reed 2005].</p></li><li><p style="padding-top: 7pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;"><span class="s19">Distribution by Region State, Users, and State-Changing Functions. </span>With the widely ac- knowledged limitations of existing virtual world architectures, ongoing work on virtual world scalability centers on additional axes of computational distribution as well as the overall system topology of multiple virtual worlds (<i>metagalaxies</i>). At this point, no single approach has yet publicly presented both a stable, widely-used reference implementation alongside empirical, quantitative evidence of significant scalability</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">improvement. A fundamental issue behind any approach to virtual world scalability is that concurrent interactions are unavoidably quadratic: for any <i>n </i>concurrently in- teracting avatars, agents, or objects—whether concurrent by location, communication, or any other relationship—there can always be on the order of <i>n</i><span class="s24">2</span> updates among that cluster of participants. Approaches to addressing this quadratic growth range from constraints on the possible interactions or number of interacting objects to replicating computations so that the same result can be reached by multiple computational units with minimal messaging.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The RedDwarf Server project, formerly Project Darkstar, takes a converse approach to scalability by allowing clients a perceived single-threaded centralized system, where the server is itself a container for distributed synchronization, data storage, and dy- namic reallocation of computational tasks [Waldo 2008; RedDwarf Server Project 2011]. The project itself is not exclusively targeted for virtual worlds, with online games and non-immersive social networks as possible applications as well. Returning briefly to online games, Trion Worlds’s <i>Rift </i>is noteworthy here because its server architecture also claims to use a similar dynamic reallocation approach as Project Darkstar along with the distribution of computational load by functions beyond shards [Gladstone 2011; Gamasutra Staff 2011].</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">SEVE (Scalable Engine for Virtual Environments) uses an action-based protocol that leverages the semantics of virtual world activities to both streamline the communica- tion between servers and clients as well as optimistically replicate state changes across all active hosts [Gupta et al. 2009]. Virtual world agents generate a queue of action– result pairs, with actions sent to the server as they are performed at each client. The role of the server is to reconcile the actions of multiple agents into a single authorita- tive <i>state of the world</i>. Successive changes to the state of the world are redistributed to affected clients periodically, with adjustments made to each client’s rendering of the world as needed. SEVE has been shown to scale well in terms of concurrent clients and complex actions, though individual clients themselves may require additional power and storage because they replicate the world state while running.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">The distributed scene graph (DSG) approach separates the state of the virtual world into multiple scenes, with computational actors each focused on specific types of changes to a scene (e.g., physics, scripts, collision detection) and communication- intensive client managers serving as liaisons between connected clients/viewers and the virtual world [Liu et al. 2010]. This separation strategy isolates distinct compu- tational concerns from the data/state of the world and communications among clients that are subscribed to that world. This approach appears to be very amenable to dy- namic load balancing, because specific types of work (e.g., physics calculations, script execution, avatar interaction) can be distributed among multiple machines, with the <i>state of the scene </i>serving as the unifying target of each server’s work. Liu et al. are also experimenting with dynamic level-of-detail reduction, allowing viewers that see broad areas of a virtual world to receive and process less scene data—an optimization sug- gested by work on realism and perception [Blascovich and Bailenson 2011; Glencross et al. 2006]. The system is implemented on top of OpenSim.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li><p class="s19" style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Proposed Architectural Direction. <span class="p">The constraints and requirements of virtual world scalability, including the need to maximize distribution in the face of limited bandwidth, have constituted a long-standing challenge to the field [Morningstar and Farmer 1991]. All told, although no single system has yet established itself as a clear next step, the components needed for such a system appear to have emerged among the various works surveyed in this section.</span></p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">In terms of an overall strategy, the distributed scene graph (DSG) architecture offers the broadest degree of distribution, the performance of which has been supported by</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">an OpenSim-based prototype and preliminary studies [Liu et al. 2010]. The core of this architecture, which is a set of scene servers whose sole responsibility is to manage the state of a specific region within the virtual world, may best be implemented using a system such as RedDwarf, whose features and strengths correspond well with the role of a DSG scene server. With RedDwarf-driven scene servers or grids, scene management itself can scale internally based on the size and activity within a particular section of the virtual world. Although no specific recommendations can be made for the other components of the DSG strategy at this time—physics engines, scripting machines, user managers, etc.—the very separation of other virtual world functions across these broad categories facilitates open innovation and experimentation in these areas.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Assuming that a standard <i>scene transaction </i>protocol exists for scene servers (whether DSG’s, RedDwarf’s, or any other functionality sufficient specification), another key ar- chitectural element would be the dynamic, open-ended capabilities afforded by the peer- to-peer (P2P) approach taken by Solipsis and OpenCobalt. P2P complements DSG very well in a variety of ways: first, P2P facilitates open growth of a virtual world system by allowing new scene servers (and thus new places) to join it at any time from anywhere on the Internet. An architecture that includes the replicated computation functionality of OpenCobalt will also allow individual scenes to accrue additional servers as needed for particularly busy or heavily-populated periods.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">DSG’s distribution across multiple axes (region, user, function) also benefits greatly from P2P. With the separation of scripting servers from scene servers, for example, P2P will permit users’ own machines, or others, to participate in the execution of scripts, thus distributing that computational load quite effectively. The ability to add more client managers as a particular concurrent user population grows addresses that particular axis of virtual world scalability. As mentioned earlier, P2P issues are not entirely technical; successful establishment of P2P in virtual worlds will require that no artificially restrictive policies prevent other systems from joining or contributing to the Metaverse as needed.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Where the DSG strategy addresses the distribution of computational load, the com- munication load, which increases quadratically as the number of peers increases, can be addressed by area of interest (AOI) or other clustering techniques, or by reducing or coalescing the information that is exchanged by servers, as seen in SEVE’s seman- tic action-based protocol and others. This final piece completes the overall proposed blueprint for a fully scalable Metaverse architecture, illustrated in Figure 3 and de- rived from Figure 5 of Liu et al. [2010].</p></li></ol></li></ol></li><li><h2 style="padding-top: 9pt;padding-left: 16pt;text-indent: -11pt;text-align: left;">SUMMARY AND CONCLUSIONS</h2></li></ol><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">In the past three decades, considerable progress has been made in moving from text- based multiuser virtual environments to the technical implementation of advanced virtual worlds that previously existed only in the literary imagination. Contemporary virtual worlds are now complex immersive environments with increasingly realistic 3D graphics, integrated spatial voice, content creation tools, and advanced economies. These progressive capabilities enable them to serve as elaborate contexts for work, socialization, creativity, and play and to increasingly operate more like digital cultures than as games.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Virtual world development now faces a major new challenge: how to move from a set of sophisticated but completely independent immersive environments to a massive integrated network of 3D virtual worlds or Metaverse, thus establishing a parallel context for human interaction and culture. The current work has defined success in this effort as achieving significant progress with regard to four features that are con- sidered central elements of a fully-realized Metaverse: realism (enabling users to feel fully immersed in an alternative realm), ubiquity (establishing access to the system</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 60pt;text-indent: 0pt;text-align: left;"><span><img width="382" height="353" alt="image" src="2480741.2480751_files/Image_017.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 81pt;text-indent: 0pt;text-align: left;">Fig. 3<span class="s2">. Proposed architectural direction for a scalable Metaverse.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">via all existing digital devices and maintaining the user’s virtual identity throughout all transitions within the system), interoperability (allowing 3D objects to be created and moved anywhere and users to have seamless uninterrupted movement through- out the system), and scalability (permitting concurrent efficient use of the system by massive numbers of users). The current status and needed developments to move to a fully functioning Metaverse with respect to each of these essential features can be summarized as follows.</p><p class="s4" style="padding-top: 6pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Realism. <span class="p">Technologies and techniques for improving immersive realism, especially for the senses of sight, hearing, and touch, continue to evolve independently of vir- tual worlds due to their applications in other domains, such as games, entertainment, and social media. Bringing these developments into the Metaverse is typically a phase behind innovations in these other contexts due to the additional requirement for bidi- rectional information flow (i.e., input and output) and real-time performance. Moving forward, realism-related work must achieve either concurrent input/output capability or real-time performance, or both, in order to achieve a useful role in virtual world applications. In addition, developments for all senses must be integrated to form a co- hesive immersive whole, exemplified by the challenge presented by the uncanny valley. Crossing the uncanny valley serves as an endpoint toward which innovations in virtual world realism continue to strive and possibly lead, because of the greater impact that psychological engagement has on virtual world realism than, say, realism in visual effects or computer games.</span></p><p class="s4" style="padding-top: 7pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Ubiquity. <span class="p">The first aspect of ubiquity—ubiquitous availability and access to virtual worlds—rides on the crest of developments in ubiquitous computing in general. As ubicomp has progressed, access to virtual worlds has begun to move beyond a stationary</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">desktop PC rig, expanding now into laptops, tablets, mobile devices, and augmented reality. In addition, the evolution of Web browsers from static document viewers into programmable 3D engines in their own right can potentially reduce the barrier to entry into a virtual world environment to a single addressable link, without requiring time-consuming software installation or updates/maintenance. The second aspect— ubiquitous identity or <i>manifest persona</i>—has emerged as multiple avenues of digital expression (blogs, social networks, photo/video hosting, etc.) have become increasingly widespread. These artifacts extrapolate naturally into virtual world artifacts, such as virtual clothing, objects, and buildings, but currently suffer from balkanization across multiple user accounts and information hubs that integrate only incidentally through the individual or individuals who know the usernames and passwords to each of these venues.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Progress in ubiquitous availability and access to virtual worlds may very well take care of itself, as the increasing power, diversity, and availability of general comput- ing devices as well as the capabilities of browser-based implementations continue to advance. As long as virtual world developments move alongside general ubicomp developments, moving in and out of the Metaverse may become as convenient and fluid as browsing the Worldwide Web is today. In contrast, achieving ubiquitous iden- tity will require more concerted and coordinated effort, both technologically and so- cially/politically, to allow one’s information to flow more freely among various milieus. Social networks are beginning to move in this direction though have not fully reached it yet, and virtual worlds must follow suit in this area. It is also possible that virtual worlds may take a leadership position in this regard because virtual world artifacts may be more closely linked to one’s digital persona due to the immersive environ- ment (and therefore may motivate progress in this area more than social networks or blog/community sites can).</p><p class="s4" style="padding-top: 6pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Interoperability. <span class="p">Interoperability in virtual worlds currently exists as a loosely con- nected collection of information, format, and data standards, most of which focus on the transfer of 3D models/objects across virtual world environments. Although many standards have been proposed (e.g., VRML, X3D, COLLADA), no single one has gained long-term traction (though COLLADA is showing some promise). However, virtual world interoperability is not solely limited to 3D object transfer: true interoperability also involves communication protocol, locator, identity, and currency standards (with the first two having direct equivalents in the Worldwide Web). The widespread adoption of standards in any area of computing is frequently borne of much work, collaboration, and advocacy, where the most difficult barriers may not actually be technological or computational but social or political. Accomplishing virtual world interoperability is no different and in some ways is exacerbated by the sheer breadth of elements and layers that must transfer seamlessly across different virtual world systems. Candidate standards exist; however, they are either in primary use outside of virtual worlds or else remain preliminary and are still very much works in progress. The wide-ranging requirements and scope of digital assets involved in virtual worlds have the potential of making the Metaverse the “killer app” that finally leads the charge toward seamless interoperability.</span></p><p class="s4" style="padding-top: 6pt;padding-left: 5pt;text-indent: 9pt;text-align: justify;">Scalability. <span class="p">Virtual world technologies are currently in an initial stage of departure from highly centralized system architectures, with the distribution of load being pre- dominantly focused on contiguous regions or areas. The limitations of this strategy have been identified, and multiple independent initiatives have proposed potential next steps ranging from peer-to-peer approaches to divisions of computational and communication load in ways that address the particular scalability needs of virtual world environments [Liu et al. 2010]. Going forward, an integrative phase is needed</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">where the multiple independent research threads are brought together in complemen- tary and cohesive ways to form a whole that is greater than the sum of its parts. From the surveyed work, an overall architectural strategy based on DSG (with a high- performance “world state” system like RedDwarf at its core), peer-to-peer openness as seen in Open Cobalt and Solipsis for broad distribution of load (not an exclusively tech- nological challenge), and reduction/optimization techniques like action-based protocols appears to hold the most promise for massive scaling of an integrated virtual world system. As echoed in the NAE Grand Challenges, this is scaling of the highest order: Progress in virtual world scalability implies progress in the scalability of many other types of multiuser multitiered systems.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Several factors promote optimism that a fully developed Metaverse can be achieved, alongside a number of significant constraints to realizing this goal. On the encour- aging side, the National Academy of Engineering’s designation of enhanced virtual reality (with virtual worlds a major subset of this field) as one of the 14 grand challenges for engineering in the 21st century [National Academy of Engineering 2008] along with government reports from China and Japan identifying virtual re- ality as a priority technology for development [Zhao 2011] provides a strong im- petus from major institutional sources for continued innovation in the field. An- other source of encouragement comes from the continued expansion of public in- terest in the use of virtual worlds. According to KZero, a virtual world monitoring service, the number of worldwide registered accounts in virtual worlds approached</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="http://www.slideshare.net/nicmitham/kzero-universe-q2-2011)" class="s5" target="_blank">1.4 billion as of the end of the second quarter of 2011, representing an overall growth rate of 338% from approximately 414 million global accounts in slightly over two years (</a>http://www.slideshare.net/nicmitham/kzero-universe-q2-2011). Moreover, approximately 70% of current account holders are below the age of 16, with almost half (652 million) in the 10–15 year old segment, and only 3% of worldwide users over the age of 25. These data indicate that a new generation accustomed to graphically-rich 3D digital environments (both virtual worlds and immersive games offered online and through consoles, such as PlayStation, Xbox360, and Wii) is rapidly coming of age and will likely fuel continued development in all immersive digital platforms, including advanced virtual worlds.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Finally, continued improvement in hardware performance, as reflected in Moore’s law and Kurzweil’s law of accelerating returns [Moore 1965; Kurzweil 2001], promotes confidence that sufficient computing efficiency will be available to achieve real-time performance of any computable function related to the operation of virtual worlds. In sum, top-down forces (i.e., the priority focus of scientific and governmental insti- tutions), bottom-up pressures stemming from expanding popular interest in immer- sive digital environments, and continued advances in hardware capacity and perfor- mance all contribute to positive expectations for progress in achieving a fully-realized Metaverse.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">Along with forces that are propelling the development of the Metaverse forward, there are two significant barriers that may inhibit the pace or extent of this progress. The first pertains to current limits in computational methods related to virtual worlds. It is noteworthy that most of the items contained in Zhao’s delineation of the major challenges facing virtual world development (such as determining whether it is possi- ble to digitally model any given object in the physical world—a <i>theory of modelability </i>or whether a duality/trade-off truly exists between virtual environment fidelity and real- time performance) focus on computational issues (i.e., the development of algorithms, theories, concepts, and paradigms) rather than hardware problems [Zhao 2011]. In fact, based upon current understanding, the computability of many of these key processes is presently unknown and thus conceptual advances will play a pivotal role in future progress toward creating a viable Metaverse. For example, because most of the major</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">algorithms related to immersive realism have been developed and established, signifi- cant progress in real-time performance and bidirectional information flow of elements that contribute to this feature is likely in the immediate future. In contrast, in the area of scalability, conceptual solutions related to the optimum architectural configuration are still being considered, and until a consensus is achieved on this issue, it is unlikely that there will be rapid progress in achieving a massively scaled Metaverse.</p><p style="padding-left: 5pt;text-indent: 9pt;text-align: justify;">In addition to conceptual and computational challenges, the development of the Metaverse may be constrained by significant economic and political barriers. Cur- rently, virtual worlds are dominated by proprietary platforms, such as Second Life, Cryworld, Utherverse, IMVU, and World of Warcraft, or government-controlled worlds, such as the China-based Hipihi. These platforms have played an important role in the history of virtual worlds by providing development capital and enhancing public awareness of the technology. However, just as the old walled gardens of AOL, Com- puServe, and Prodigy were instrumental in expanding Internet usage early on but ultimately became an inhibitory force in the development of the Worldwide Web, these proprietary and state-based virtual world platforms have sparked initial growth but now risk constraining innovation and advancement. As we have seen with the devel- opment of the Internet, progress has been best served by the combined participation and innovation of proprietary and open-source initiatives. Similarly, the advancement of a fully-realized Metaverse would likely be maximized by harnessing the same pro- cess of collective effort and mass innovation that was instrumental in the creation and expansion of the Web.</p><h2 style="padding-top: 10pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">REFERENCES</h2><p class="s2" style="padding-top: 4pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">A<span class="s18">BATE</span>, A. F., G<span class="s18">UIDA</span>, M., L<span class="s18">EONCINI</span>, P., N<span class="s18">APPI</span>, M., <span class="s25">AND </span>R<span class="s18">ICCIARDI</span>, S. 2009. A haptic-based approach to virtual training for aerospace industry. <i>J. Vis. Lang. Comput. 20</i>, 318–325.</p><p class="s6" style="text-indent: 0pt;line-height: 8pt;text-align: left;">=</p><p style="text-indent: 0pt;text-align: left;"/><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">A<span class="s18">CTION</span><span class="s25"> </span>R<span class="s18">EACTION</span><span class="s25"> </span>L<span class="s18">ABS</span><a href="http://www.actionreactionlabs.com/" class="a" target="_blank">, L. 2010. Ghost dynamic binaural audio. </a>http://www.actionreactionlabs.com/ ghost.php. (Last accessed on 3/11).</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">A<span class="s18">DABALA</span>, N., M<span class="s18">AGNENAT</span>-T<span class="s18">HALMANN</span>, N., <span class="s25">AND </span>F<span class="s18">EI</span>, G. 2003. Real-time rendering of woven clothes. In <i>Proceedings of the ACM Symposium on Virtual Reality Software and Technology (VRST’03)</i>. ACM, New York, NY, 41–47.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">A<span class="s18">LEXANDER</span>, O., R<span class="s18">OGERS</span>, M., L<span class="s18">AMBETH</span>, W., C<span class="s18">HIANG</span>, M., <span class="s25">AND </span>D<span class="s18">EBEVEC</span>, P. 2009. The digital emily project: Photoreal facial modeling and animation. In <i>Proceedings of the 36</i><i>th</i><i> International Conferece and Exhibition on Computer Graphics and Interactive Technologies (SIGGRAPH’09)</i>. ACM, New York, NY, 12:1–12:15.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">A<span class="s18">NTONELLI</span><span class="s25"> </span>(AV), J. <span class="s25">AND </span>M<span class="s18">AXSTED</span><span class="s25"> </span>(AV), M. 2011. Kokua/Imprudence viewer website. http://www.kokuaviewer. org. (Last accessed 3/11).</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">A<span class="s18">RANGARASAN</span>, R. <span class="s25">AND </span>P<span class="s18">HILLIPS</span><span class="s25"> </span>J<span class="s18">R</span>., G. N. 2002. Modular approach of multimodal integration in a virtual envi- ronment. In <i>Proceedings of the 4th IEEE International Conference on Multimodal Interfaces (ICMI’02)</i>. IEEE Computer Society, Washington, DC, 331–336.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">A<span class="s18">RNAUD</span>, R. <span class="s25">AND </span>B<span class="s18">ARNES</span>, M. C. 2006. <i>COLLADA: Sailing the Gulf of 3D Digital Content Creation</i>. A K Peters Ltd, Natick, MA.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">A<span class="s18">SPIN</span>, R. A. <span class="s25">AND </span>R<span class="s18">OBERTS</span>, D. J. 2011. A GPU based, projective multi-texturing approach to reconstructing the 3D human form for application in tele-presence. In <i>Proceedings of the ACM Conference on Computer Supported Cooperative Work (CSCW’11)</i>. ACM, New York, NY, 105–112.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">A<span class="s18">U</span>, W. 2008. <i>The Making of Second Life</i>. Collins, New York, NY.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">A<span class="s18">URORA</span>-S<span class="s18">IM</span><span class="s25"> </span>P<span class="s18">ROJECT</span><a href="http://www.aurora-sim.org/" class="a" target="_blank">. 2011. Aurora-Sim website. </a>http://www.aurora-sim.org. (Last accessed 3/11).</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">B<span class="s18">AILLIE</span>, L., K<span class="s18">UNCZIER</span>, H., <span class="s25">AND </span>A<span class="s18">NEGG</span>, H. 2005. Rolling, rotating and imagining in a virtual mobile world. In <i>Proceedings of the 7th International Conference on Human Computer Interaction with Mobile Devices &amp; Services (MobileHCI’05)</i>. ACM, New York, NY, 283–286.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">B<span class="s18">AKAUS</span><a href="http://www.youtube.com/watch?v=Ol3qQ4CEUTo" class="a" target="_blank">, P. 2010a. Aves engine sneak preview. </a>http://www.youtube.com/watch?v=Ol3qQ4CEUTo. (Last ac- cessed on 6/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">B<span class="s18">AKAUS</span>, P. 2010b. Building a game engine with jQuery. In <i>Proceedings of the jQuery Conference</i>. http:</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a href="http://www.slideshare.net/pbakaus/building-a-game-engine-with-jquery" class="a" target="_blank">//www.slideshare.net/pbakaus/building-a-</a>game-engine-with-jquery. (Last accessed 7/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">B<span class="s18">ERNERS</span>-L<span class="s18">EE</span>, T., F<span class="s18">IELDING</span>, R., <span class="s25">AND </span>M<span class="s18">ASINTER</span>, L. 2005. Uniform resource identifier (URI): Generic syntax.</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: justify;"><a href="http://www.ietf.org/rfc/rfc2396.txt" class="a" target="_blank">Internet STD 66, RFC 3986. http://www.ietf.org/rfc/rfc2396.txt.</a> (Last accessed 7/11).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-top: 5pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">B<span class="s18">IOCCA</span>, F., K<span class="s18">IM</span>, J., <span class="s25">AND </span>C<span class="s18">HOI</span>, Y. 2001. Visual touch in virtual environments: An exploratory study of presence, multimodal interfaces, and cross-modal sensory illusions. <i>Presence: Teleoper. Virtual Environ. 10</i>, 247– 265.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">B<span class="s18">LANCHARD</span>, C., B<span class="s18">URGESS</span>, S., H<span class="s18">ARVILL</span>, Y., L<span class="s18">ANIER</span>, J., L<span class="s18">ASKO</span>, A., O<span class="s18">BERMAN</span>, M., <span class="s25">AND </span>T<span class="s18">EITEL</span>, M. 1990. Reality built</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">for two: A virtual reality tool. <i>SIGGRAPH Comput. Graph. 24</i>, 35–36.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -16pt;text-align: justify;">B<span class="s18">LASCOVICH</span>, J. <span class="s25">AND </span>B<span class="s18">AILENSON</span>, J. 2011. <i>Infinite Reality: Avatars, Eternal Life, New Worlds, and the Dawn of the Virtual Revolution</i>. William Morrow, New York, NY.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">B<span class="s18">LAUERT</span>, J. 1996. <i>Spatial Hearing: The Psychophysics of Human Sound Localization</i>. MIT Press, Cambridge, MA.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">B<span class="s18">OELLSTORFF</span>, T. 2008. <i>Coming of Age in Second Life: An Anthropologist Explores the Virtually Human</i>.</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Princeton University Press, Princeton, NJ.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">B<span class="s18">ONNEEL</span>, N., D<span class="s18">RETTAKIS</span>, G., T<span class="s18">SINGOS</span>, N., V<span class="s18">IAUD</span>-D<span class="s18">ELMON</span>, I., <span class="s25">AND </span>J<span class="s18">AMES</span>, D. 2008. Fast modal sounds with scalable</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">frequency-domain synthesis. <i>ACM Trans. Graph. 27</i>, 24:1–24:9.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">B<span class="s18">OULANGER</span>, K., P<span class="s18">ATTANAIK</span>, S. N., <span class="s25">AND </span>B<span class="s18">OUATOUCH</span>, K. 2009. Rendering grass in real time with dynamic lighting.</p><p class="s3" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">IEEE Comput. Graph. Appl. 29<span class="s2">, 32–41.</span></p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">B<span class="s18">OUTHORS</span>, A., N<span class="s18">EYRET</span>, F., M<span class="s18">AX</span>, N., B<span class="s18">RUNETON</span>, E., <span class="s25">AND </span>C<span class="s18">RASSIN</span>, C. 2008. Interactive multiple anisotropic scattering in clouds. In <i>Proceedings of the Symposium on Interactive 3D Graphics and Games (I3D’08)</i>. ACM, New York, NY, 173–182.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">B<span class="s18">RADLEY</span>, D., H<span class="s18">EIDRICH</span>, W., P<span class="s18">OPA</span>, T., <span class="s25">AND </span>S<span class="s18">HEFFER</span>, A. 2010. High resolution passive facial performance capture.</p><p class="s3" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">ACM Trans. Graph. 29<span class="s2">, 41:1–41:10.</span></p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">B<span class="s18">ROWN</span>, C. P. <span class="s25">AND </span>D<span class="s18">UDA</span>, R. O. 1998. A structural model for binaural sound synthesis. <i>IEEE Trans. Speech Audio Process. 6</i>, 476–488.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -16pt;text-align: left;">B<span class="s18">ULLION</span>, C. <span class="s25">AND </span>G<span class="s18">UROCAK</span>, H. 2009. Haptic glove with mr brakes for distributed finger force feedback. <i>Presence: Teleoper. Virtual Environ. 18</i>, 421–433.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">B<span class="s18">URNS</span><a href="http://cityofnidus.blogspot.com/2010/04/" class="a" target="_blank">, W. 2010. Defining the metaverse — revisited. </a>http://cityofnidus.blogspot.com/2010/04/ defining-metaverse-revisited.html. (Last accessed 5/11).</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">C<span class="s18">HADWICK</span>, J. N., A<span class="s18">N</span>, S. S., <span class="s25">AND </span>J<span class="s18">AMES</span>, D. L. 2009. Harmonic shells: A practical nonlinear sound model for near-rigid thin shells. <i>ACM Trans. Graph. 28</i>, 119:1–119:10.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">C<span class="s18">HANDRASIRI</span>, N. P., N<span class="s18">AEMURA</span>, T., I<span class="s18">SHIZUKA</span>, M., H<span class="s18">ARASHIMA</span>, H., <span class="s25">AND </span>B<span class="s18">ARAKONYI</span>, I. 2004. Internet communication</p><p class="s2" style="padding-left: 5pt;text-indent: 15pt;line-height: 119%;text-align: left;">using real-time facial expression analysis and synthesis. <i>IEEE MultiMedia 11</i>, 20–29. C<span class="s18">HARLES</span>, J. 1999. Neural interfaces link the mind and the machine. <i>Computer 32</i>, 16–18.</p><p class="s2" style="padding-left: 21pt;text-indent: -15pt;text-align: justify;">C<span class="s18">HOW</span>, Y.-W., P<span class="s18">OSE</span>, R., R<span class="s18">EGAN</span>, M., <span class="s25">AND </span>P<span class="s18">HILLIPS</span>, J. 2006. Human visual perception of region warping distortions. In <i>Proceedings of the 29th Australasian Computer Science Conference - Volume 48 (ACSC’06)</i>. Australian Computer Society, Inc., Darlinghurst, Australia, 217–226.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">C<span class="s18">HUN</span>, J., K<span class="s18">WON</span>, O., M<span class="s18">IN</span>, K., <span class="s25">AND </span>P<span class="s18">ARK</span>, P. 2007. Real-time face pose tracking and facial expression synthesizing for the animation of 3D avatar. In <i>Proceedings of the 2nd International Conference on Technologies for e-Learning and Digital Entertainment (Edutainment’07)</i>. Springer-Verlag, Berlin, Heidelberg, 191–201.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">C<span class="s18">INQUETTI</span><span class="s25"> </span><a href="http://www.kirstensviewer.com/" class="a" target="_blank">(AV), K. 2011. Kirstens viewer website. </a>http://www.kirstensviewer.com. (Last accessed 2/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -16pt;text-align: justify;">C<span class="s18">OBOS</span>, M., L<span class="s18">OPEZ</span>, J. J., <span class="s25">AND </span>S<span class="s18">PORS</span>, S. 2010. A sparsity-based approach to 3D binaural sound synthesis using time-frequency array processing. <i>EURASIP J. Adv. Signal Proces.</i></p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">COLLADA C<span class="s18">OMMUNITY</span><a href="http://collada.org/" class="a" target="_blank">. 2011. COLLADA: Digital asset and FX exchange schema wiki. </a>http://collada.org. (Last accessed 5/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">C<span class="s18">OZZI</span>, L., D’A<span class="s18">NGELO</span>, P., C<span class="s18">HIAPPALONE</span>, M., I<span class="s18">DE</span>, A. N., N<span class="s18">OVELLINO</span>, A., M<span class="s18">ARTINOIA</span>, S., <span class="s25">AND </span>S<span class="s18">ANGUINETI</span>, V. 2005.</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Coding and decoding of information in a bi-directional neural interface. <i>Neurocomput. 65–66</i>, 783–792.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">C<span class="s18">RUZ</span>-N<span class="s18">EIRA</span>, C., S<span class="s18">ANDIN</span>, D. J., D<span class="s18">E</span>F<span class="s18">ANTI</span>, T. A., K<span class="s18">ENYON</span>, R. V., <span class="s25">AND </span>H<span class="s18">ART</span>, J. C. 1992. The cave: Audio visual</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">experience automatic virtual environment. <i>Commun. ACM 35</i>, 64–72.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;"><span class="s25">DE </span>P<span class="s18">ASCALE</span>, M., M<span class="s18">ULATTO</span>, S., <span class="s25">AND </span>P<span class="s18">RATTICHIZZO</span>, D. 2008. Bringing haptics to Second Life. In <i>Proceedings of the Ambi-Sys Workshop on Haptic User Interfaces in Ambient Media Systems (HAS’08)</i>. ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering), ICST, Brussels, Belgium, 6:1–6:6.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -16pt;text-align: justify;">D<span class="s18">EAN</span><span class="s25"> </span><a href="http://sourceforge.net/projects/niransviewer.(Last" class="a" target="_blank">(AV), N. 2011. Nirans viewer project site. http://sourceforge.net/projects/niransviewer. </a>(Last accessed 12/11).</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">E<span class="s18">LEK</span>, O. <span class="s25">AND </span>K<span class="s18">MOCH</span>, P. 2010. Real-time spectral scattering in large-scale natural participating media. In <i>Proceedings of the 26th Spring Conference on Computer Graphics (SCCG’10)</i>. ACM, New York, NY, 77–84.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">F<span class="s18">ACEBOOK</span><a href="http://developers.facebook.com/" class="a" target="_blank">. 2011. Facebook developers website. </a>http://developers.facebook.com. (Last accessed 5/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">F<span class="s18">ISHER</span>, B., F<span class="s18">ELS</span>, S., M<span class="s18">AC</span>L<span class="s18">EAN</span>, K., M<span class="s18">UNZNER</span>, T., <span class="s25">AND </span>R<span class="s18">ENSINK</span>, R. 2004. Seeing, hearing, and touching: Putting it all together. In <i>ACM SIGGRAPH 2004 Course Notes (SIGGRAPH’04)</i>. ACM, New York, NY.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-top: 5pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">F<span class="s18">OLGHERAITER</span>, M., G<span class="s18">INI</span>, G., <span class="s25">AND </span>V<span class="s18">ERCESI</span>, D. 2008. A multi-modal haptic interface for virtual reality and robotics. <i>J. Intell. Robotics Syst. 52</i>, 465–488.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">F<span class="s18">RANCONE</span>, J. <span class="s25">AND </span>N<span class="s18">IGAY</span><a href="http://iihm/" class="a" target="_blank">, L. 2011. 3D displays on mobile devices: Head-coupled perspective. </a>http://iihm. imag.fr/en/demo/hcpmobile. (Last accessed 4/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">F<span class="s18">REY</span>, D., R<span class="s18">OYAN</span>, J., P<span class="s18">IEGAY</span>, R., K<span class="s18">ERMARREC</span>, A.-M., A<span class="s18">NCEAUME</span>, E., <span class="s25">AND </span>F<span class="s18">ESSANT</span>, F. L. 2008. Solipsis: A decentral-</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;text-align: left;">ized architecture for virtual environments. In <i>Proceedings of the 1st International Workshop on Massively Multiuser Virtual Environments (MMVE)</i>. 29–33.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">G<span class="s18">AMASUTRA</span><span class="s25"> </span>S<span class="s18">TAFF</span><a href="http://www.gamasutra.com/view/news/" class="a" target="_blank">. 2011. Behind the scenes of Trion Worlds’ Rift. </a>http://www.gamasutra.com/view/news/ 35009/Exclusive_Behind_The_Scenes_Of_Trion_Worlds_Rift.php. (Last accessed 6/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 119%;text-align: left;">G<span class="s18">EARZ</span><span class="s25"> </span><a href="http://www.singularityviewer.org/" class="a" target="_blank">(AV), S. 2011. Singularity viewer website. http://www.singularityviewer.org.</a> (Last accessed 3/11). G<span class="s18">EDDES</span>, L. 2010. Immortal avatars: Back up your brain, never die. <i>New Scientist 206, </i>2763, 28–31.</p><p class="s2" style="padding-left: 21pt;text-indent: -15pt;text-align: left;">G<span class="s18">HOSH</span>, A. 2007. Realistic materials and illumination environments. Ph.D. dissertation, University of British Columbia, Vancouver, BC, Canada. AAINR31775.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">G<span class="s18">ILBERT</span>, R. L. 2011. The P.R.O.S.E. Project: A program of in-world behavioral research on the Metaverse.</p><p class="s3" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">J. Virtual Worlds Res. 4, <span class="s2">1, 3–18.</span></p><p class="s2" style="padding-top: 1pt;text-indent: 0pt;line-height: 9pt;text-align: right;">G<span class="s18">ILBERT</span>, R. L. <span class="s25">AND </span>F<span class="s18">ORNEY</span>, A. 2013. The distributed self: Virtual worlds and the future of human identity. In</p><p class="s3" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: right;">The Immersive Internet: Reflections on the entagling of the virtual with society, politics, and the economy<span class="s2">,</span></p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">D. Powers and R. Teigland, Eds., Palgrave-Macmillan, 23–37.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">G<span class="s18">ILBERT</span>, R. L., F<span class="s18">OSS</span>, J., <span class="s25">AND </span>M<span class="s18">URPHY</span>, N. 2011a. Multiple personality order: Physical and personality character- istics of the self, primary avatar, and alt. In <i>Reinventing Ourselves: Contemporary Concepts of Identity in Virtual Worlds</i>, A. Peachey and M. Childs, Eds. Springer Publishing, London, 213–234.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">G<span class="s18">ILBERT</span>, R. L., G<span class="s18">ONZALEZ</span>, M. A., <span class="s25">AND </span>M<span class="s18">URPHY</span>, N. A. 2011b. Sexuality in the 3D Internet and its relationship to real-life sexuality. <i>Psychol. Sexual. 2</i>, 107–122.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">G<span class="s18">LADSTONE</span><a href="http://h20435.www2.hp.com/t5/" class="a" target="_blank">, D. 2011. Trion rift opens on HP servers. The next bench blog, </a>http://h20435.www2.hp.com/t5/ The-Next-Bench-Blog/Trion-Rift-Opens-on-HP-Servers/ba-p/61333. (Last accessed on 6/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">G<span class="s18">LENCROSS</span>, M., C<span class="s18">HALMERS</span>, A. G., L<span class="s18">IN</span>, M. C., O<span class="s18">TADUY</span>, M. A., <span class="s25">AND  </span>G<span class="s18">UTIERREZ</span>, D. 2006. Exploiting perception</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;text-align: left;">in high-fidelity virtual environments. In <i>ACM SIGGRAPH 2006 Courses (SIGGRAPH’06)</i>. ACM, New York, NY.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">G<span class="s18">OSSELIN</span>, B. <span class="s25">AND </span>S<span class="s18">AWAN</span>, M. 2010. A low-power integrated neural interface with digital spike detection and extraction. <i>Analog Integr. Circuits Signal Process. 64</i>, 3–11.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">G<span class="s18">REENBERG</span>, S., M<span class="s18">ARQUARDT</span>, N., B<span class="s18">ALLENDAT</span>, T., D<span class="s18">IAZ</span>-M<span class="s18">ARINO</span>, R., <span class="s25">AND </span>W<span class="s18">ANG</span>, M. 2011. Proxemic interactions: The</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">new ubicomp? <i>Interactions 18</i>, 42–50.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">G<span class="s18">UPTA</span>, N., D<span class="s18">EMERS</span>, A., G<span class="s18">EHRKE</span>, J., U<span class="s18">NTERBRUNNER</span>, P., <span class="s25">AND </span>W<span class="s18">HITE</span>, W. 2009. Scalability for virtual worlds. In</p><p class="s3" style="padding-left: 4pt;text-indent: 0pt;line-height: 9pt;text-align: center;">Proceedings of the IEEE 25th International Conference on Data Engineering (ICDE’09)<span class="s2">. 1311–1314.</span></p><p class="s2" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;line-height: 9pt;text-align: center;">H<span class="s18">AANS</span>, A. <span class="s25">AND </span>IJ<span class="s18">SSELSTEIJN</span>, W. 2006. Mediated social touch: A review of current research and future directions.</p><p class="s3" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: center;">Virtual Real. 9<span class="s2">, 149–159.</span></p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">H<span class="s18">AO</span>, A., S<span class="s18">ONG</span>, F., L<span class="s18">I</span>, S., L<span class="s18">IU</span>, X., <span class="s25">AND </span>X<span class="s18">U</span>, X. 2009. Real-time realistic rendering of tissue surface with mucous layer. In <i>Proceedings of the 2nd International Workshop on Computer Science and Engineering - Volume 01 (IWCSE’09)</i>. IEEE Computer Society, Washington, DC, 302–306.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">H<span class="s18">ARDERS</span>, M., B<span class="s18">IANCHI</span>, G., <span class="s25">AND </span>K<span class="s18">NOERLEIN</span>, B. 2007. Multimodal augmented reality in medicine. In <i>Proceed- ings of the 4th International Conference on Universal Access in Human-Computer Interaction: Ambient Interaction (UAHCI’07)</i>. Springer-Verlag, Berlin, Heidelberg, 652–658.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">H<span class="s18">AYHOE</span>, M. M., B<span class="s18">ALLARD</span>, D. H., T<span class="s18">RIESCH</span>, J., S<span class="s18">HINODA</span>, H., A<span class="s18">IVAR</span>, P., <span class="s25">AND </span>S<span class="s18">ULLIVAN</span>, B. 2002. Vision in natural</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;text-align: justify;">and virtual environments. In <i>Proceedings of the Symposium on Eye Tracking Research &amp; Applications (ETRA’02)</i>. ACM, New York, NY, 7–13.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">H<span class="s18">ERTZMANN</span>, A., O’S<span class="s18">ULLIVAN</span>, C., <span class="s25">AND </span>P<span class="s18">ERLIN</span>, K. 2009. Realistic human body movement for emotional expres- siveness. In <i>ACM SIGGRAPH 2009 Courses (SIGGRAPH’09)</i>. ACM, New York, NY, 20:1–20:27.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">H<span class="s18">O</span>, C.-C., M<span class="s18">AC</span>D<span class="s18">ORMAN</span>, K. F., <span class="s25">AND </span>P<span class="s18">RAMONO</span>, Z. A. D. D. 2008. Human emotion and the uncanny valley: A GLM, MDS, and Isomap analysis of robot video ratings. In <i>Proceedings of the 3rd ACM/IEEE International Conference on Human Robot Interaction (HRI’08)</i>. ACM, New York, NY, 169–176.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">H<span class="s18">U</span>, Y., V<span class="s18">ELHO</span>, L., T<span class="s18">ONG</span>, X., G<span class="s18">UO</span>, B., <span class="s25">AND </span>S<span class="s18">HUM</span>, H. 2006. Realistic, real-time rendering of ocean waves: Research articles. <i>Comput. Animat. Virtual Worlds 17</i>, 59–67.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">IEEE VW S<span class="s18">TANDARD</span><span class="s25"> </span>W<span class="s18">ORKING</span><span class="s25"> </span>G<span class="s18">ROUP</span><a href="http://www/" class="a" target="_blank">. 2011a. IEEE virtual world standard working group wiki. </a>http://www. metaversestandards.org. (Last accessed 5/11).</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">IEEE VW S<span class="s18">TANDARD</span><span class="s25"> </span>W<span class="s18">ORKING</span><span class="s25"> </span>G<span class="s18">ROUP</span>. 2011b. Terminology and definitions. http://www.metaversestandards. org/index.php?title=Terminology_and_Definitions#MetaWorld. (Last accessed 5/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">I<span class="s18">MAGE</span><span class="s25"> </span>M<span class="s18">ETRICS</span><a href="http://www.image-metrics.com/" class="a" target="_blank">. 2011. Image metrics website. </a>http://www.image-metrics.com. (Last accessed 4/11).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-top: 5pt;padding-left: 21pt;text-indent: -16pt;text-align: justify;">J<span class="s18">AY</span>, C., G<span class="s18">LENCROSS</span>, M., <span class="s25">AND </span>H<span class="s18">UBBOLD</span>, R. 2007. Modeling the effects of delayed haptic and visual feedback in a collaborative virtual environment. <i>ACM Trans. Comput.-Hum. Interact. 14</i>, 8:1–8:31.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">J<span class="s18">EON</span>, N. J., L<span class="s18">EEM</span>, C. S., K<span class="s18">IM</span>, M. H., <span class="s25">AND </span>S<span class="s18">HIN</span>, H. G. 2007. A taxonomy of ubiquitous computing applications.</p><p class="s3" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Wirel. Pers. Commun. 43<span class="s2">, 1229–1239.</span></p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">J<span class="s18">IMENEZ</span>, J., S<span class="s18">CULLY</span>, T., B<span class="s18">ARBOSA</span>, N., D<span class="s18">ONNER</span>, C., A<span class="s18">LVAREZ</span>, X., V<span class="s18">IEIRA</span>, T., M<span class="s18">ATTS</span>, P., O<span class="s18">RVALHO</span>, V., G<span class="s18">UTIERREZ</span>, D.,</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;text-align: justify;"><span class="s25">AND </span>W<span class="s18">EYRICH</span>, T. 2010. A practical appearance model for dynamic facial color. <i>ACM Trans. Graph. 29</i>, 141:1–141:10.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">J<span class="s18">OLY</span><a href="http://www.jonathanjoly.com/front.htm" class="a" target="_blank">, J. 2010. Can a polygon make you cry? http://www.jonathanjoly.com/front.htm.</a> (Last accessed 5/11).</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">17, 2011.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -16pt;text-align: justify;">K<span class="s18">ALL</span><span class="s25"> </span>B<span class="s18">INAURAL</span><span class="s25"> </span>A<span class="s18">UDIO</span><a href="http://www.kallbinauralaudio.com/" class="a" target="_blank">. 2010. A brief history of binaural audio. </a>http://www.kallbinauralaudio.com/ binauralhistory.html. (Last accessed 3/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">K<span class="s18">AWATO</span>, S. <span class="s25">AND </span>O<span class="s18">HYA</span>, J. 2000. Real-time detection of nodding and head-shaking by directly detecting and tracking the “between-eyes”. In <i>Proceedings of the 4th IEEE International Conference on Automatic Face and Gesture Recognition (FG’00)</i>. IEEE Computer Society, Washington, DC, 40.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">K<span class="s18">ELLER</span>, J. <span class="s25">AND </span>S<span class="s18">IMON</span>, G. 2002. Toward a peer-to-peer shared virtual reality. In <i>Proceedings of the 22nd International Conference on Distributed Computing Systems (ICDCSW’02)</i>. IEEE Computer Society, Washington, DC, 695–700.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -16pt;text-align: justify;">K<span class="s18">IM</span>,  J.-S.,  G<span class="s18">RAC</span>ˇ<span class="s18">ANIN</span>,  D.,  M<span class="s18">ATKOVIC</span>´,  K.,  <span class="s18">AND   </span>Q<span class="s18">UEK</span>,  F.  2010.  The  effects  of  finger-walking  in  place  (FWIP) for  spatial  knowledge  acquisition  in  virtual  environments.  In  <i>Proceedings  of  the  10th  International Conference on Smart Graphics (SG’10)</i>. Lecture Notes in Computer Science, vol. 6133, Springer-Verlag, Berlin, Heidelberg, 56–67.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">K<span class="s18">OEPNICK</span>, S., H<span class="s18">OANG</span>, R. V., S<span class="s18">GAMBATI</span>, M. R., C<span class="s18">OMING</span>, D. S., S<span class="s18">UMA</span>, E. A., <span class="s25">AND </span>S<span class="s18">HERMAN</span>, W. R. 2010. Graphics</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;text-align: justify;">for serious games: Rist: Radiological immersive survey training for two simultaneous users. <i>Comput. Graph. 34</i>, 665–676.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">K<span class="s18">OROLOV</span>, M. 2011. Kitely brings Facebook, instant regions to OpenSim. <i>Hypergrid Business</i>.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">K<span class="s18">ROL</span>, L. R., A<span class="s18">LIAKSEYEU</span>, D., <span class="s25">AND </span>S<span class="s18">UBRAMANIAN</span>, S. 2009. Haptic feedback in remote pointing. In <i>Proceedings of the 27th International Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA’09)</i>. ACM, New York, NY, 3763–3768.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">K<span class="s18">RUEGER</span>, M. W. 1993. <i>An Easy Entry Artificial Reality</i>. Academic Press Professional, Cambridge, MA, Chapter 7, 147–162.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -16pt;text-align: justify;">K<span class="s18">UCHENBECKER</span>, K. J. 2008. Haptography: Capturing the feel of real objects to enable authentic haptic render- ing (invited paper). In <i>Proceedings of the Ambi-Sys Workshop on Haptic User Interfaces in Ambient Media Systems (HAS’08)</i>. ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering), ICST, Brussels, Belgium, 3:1–3:3.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">K<span class="s18">UMAR</span>, S., C<span class="s18">HHUGANI</span>, J., K<span class="s18">IM</span>, C., K<span class="s18">IM</span>, D., N<span class="s18">GUYEN</span>, A., D<span class="s18">UBEY</span>, P., B<span class="s18">IENIA</span>, C., <span class="s25">AND </span>K<span class="s18">IM</span>, Y. 2008. Second life and</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">the new generation of virtual worlds. <i>Computer 41</i>, 46–53.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">K<span class="s18">URZWEIL</span><a href="http://www.kurzweilai.net/the-law-of-" class="a" target="_blank">, R. 2001. The law of accelerating returns. </a>http://www.kurzweilai.net/the-law-of- accelerating-returns. (Last accessed 4/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 119%;text-align: left;">L<span class="s18">ANIER</span>, J. 1992. Virtual reality: The promise of the future. <i>Interact. Learn. Int. 8</i>, 275–279. L<span class="s18">ANIER</span>, J. 2010. <i>You are not a Gadget: A Manifesto</i>. Alfred A. Knopf, New York, NY.</p><p class="s2" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">L<span class="s18">ANIER</span>, J. <span class="s25">AND </span>B<span class="s18">IOCCA</span>, F. 1992. An insider’s view of the future of virtual reality. <i>J. Commun. 42, </i>4, 150–172.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">L<span class="s18">ATOSCHIK</span>, M. E. 2005. A user interface framework for multimodal VR interactions. In <i>Proceedings of the 7th International Conference on Multimodal Interfaces (ICMI’05)</i>. ACM, New York, NY, 76–83.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">L<span class="s18">E</span>´<span class="s18">CUYER</span>, A. 2009. Simulating haptic feedback using vision: A survey of research and applications of pseudo- haptic feedback. <i>Presence: Teleoper. Virtual Environ. 18</i>, 39–53.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -16pt;text-align: left;">L<span class="s18">EE</span>, Y., W<span class="s18">AMPLER</span>, K., B<span class="s18">ERNSTEIN</span>, G., P<span class="s18">OPOVIC</span>´, J., <span class="s18">AND  </span>P<span class="s18">OPOVIC</span>´, Z. 2010. Motion fields for interactive character locomotion. <i>ACM Trans. Graph. 29</i>, 138:1–138:8.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">L<span class="s18">EWINSKI</span>, J. S. 2011. Light at the end of the racetrack: How Pixar explored the physics of light for <i>Cars 2</i>.</p><p class="s3" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Sci. Am., 20<span class="s2">.</span></p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">L<span class="s18">INDEN</span><span class="s25"> </span>L<span class="s18">AB</span><a href="http://wiki/" class="a" target="_blank">. 2011a. Extended Second Life knowledge base: Graphics preferences layout. </a>http://wiki. secondlife.com/wiki/Graphics_Preferences_Layout. (Last accessed 4/11).</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">L<span class="s18">INDEN</span><span class="s25"> </span>L<span class="s18">AB</span><a href="http://community/" class="a" target="_blank">. 2011b. New SL viewer with improved search and real-time shadows. </a>http://community. secondlife.com/t5/Featured-News/New-SL-Viewer-with-Improved-Search-and-Real-Time-Shadows/ ba-p/927463. (Last accessed 7/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">L<span class="s18">IU</span>, H., B<span class="s18">OWMAN</span>, M., A<span class="s18">DAMS</span>, R., H<span class="s18">URLIMAN</span>, J., <span class="s25">AND </span>L<span class="s18">AKE</span>, D. 2010. Scaling virtual worlds: Simulation require- ments and challenges. In <i>Proceedings of the Winter Simulation Conference (WSC)</i>. The WSC Foundation, Baltimore, MD, 778–790.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-top: 5pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">L<span class="s18">OBER</span>, A., S<span class="s18">CHWABE</span>, G., <span class="s25">AND </span>G<span class="s18">RIMM</span>, S. 2007. Audio vs. chat: The effects of group size on media choice. In <i>Proceedings of the 40th Annual Hawaii International Conference on System Sciences (HICSS’07)</i>. IEEE Computer Society, Washington, DC, 41.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">L<span class="s18">UDLOW</span>, P. <span class="s25">AND </span>W<span class="s18">ALLACE</span>, M. 2007. <i>The Second Life Herald: The virtual tabloid that witnessed the dawn of the Metaverse</i>. MIT Press, New York, NY.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 119%;text-align: justify;">L<span class="s18">YNN</span>, R. 2004. Ins and outs of teledildonics. <i>Wired</i><a href="http://www.wired.com/" class="a" target="_blank">. </a>http://www.wired.com/. (Last accessed 3/11). L<span class="s18">YON</span><span class="s25"> </span><a href="http://www.phoenixviewer.com/" class="a" target="_blank">(AV), J. 2011. Phoenix viewer website. </a>http://www.phoenixviewer.com. (Last accessed 3/11).</p><p class="s2" style="padding-left: 21pt;text-indent: -15pt;text-align: justify;">M<span class="s18">ARCOS</span>, S., G<span class="s18">O</span>´<span class="s18">MEZ</span>-G<span class="s18">ARC</span>´<span class="s18">IA</span>-B<span class="s18">ERMEJO</span>, J., <span class="s18">AND  </span>Z<span class="s18">ALAMA</span>, E. 2010. A realistic, virtual head for human-computer interaction. <i>Interact. Comput. 22</i>, 176–192.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">M<span class="s18">ARTIN</span>, J.-C., D’A<span class="s18">LESSANDRO</span>, C., J<span class="s18">ACQUEMIN</span>, C., K<span class="s18">ATZ</span>, B., M<span class="s18">AX</span>, A., P<span class="s18">OINTAL</span>, L., <span class="s25">AND </span>R<span class="s18">ILLIARD</span>, A. 2007. 3D</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;text-align: justify;">audiovisual rendering and real-time interactive control of expressivity in a talking head. In <i>Proceedings of the 7th International Conference on Intelligent Virtual Agents (IVA’07)</i>. Lecture Notes in Computer Science, vol. 4722, Springer-Verlag, Berlin, Heidelberg, 29–36.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">M<span class="s18">C</span>D<span class="s18">ONNELL</span>, R. <span class="s25">AND </span>B<span class="s18">REIDT</span>, M. 2010. Face reality: Investigating the uncanny valley for virtual faces. In <i>ACM SIGGRAPH ASIA 2010 Sketches (SA’10)</i>. ACM, New York, NY, 41:1–41:2.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">M<span class="s18">C</span>D<span class="s18">ONNELL</span>, R., J<span class="s18">O</span>¨ <span class="s18">RG</span>, S., M<span class="s18">C</span>H<span class="s18">UGH</span>, J., N<span class="s18">EWELL</span>, F. N., <span class="s18">AND  </span>O’S<span class="s18">ULLIVAN</span>, C. 2009. Investigating the role of body</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">shape on the perception of emotion. <i>ACM Trans. Appl. Percept. 6</i>, 14:1–14:11.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">M<span class="s18">ECWAN</span>, A. I., S<span class="s18">AVANI</span>, V. G., R<span class="s18">AJVI</span>, S., <span class="s25">AND </span>V<span class="s18">AYA</span>, P. 2009. Voice conversion algorithm. In <i>Proceedings of the International Conference on Advances in Computing, Communication and Control (ICAC3’09)</i>. ACM, New York, NY, 615–619.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 119%;text-align: justify;">M<span class="s18">ISSELHORN</span>, C. 2009. Empathy with inanimate objects and the uncanny valley. <i>Minds Mach. 19</i>, 345–359. M<span class="s18">OORE</span>, G. E. 1965. Cramming more components into integrated circuits. <i>Electronics 38, </i>8.</p><p class="s2" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">M<span class="s18">ORI</span>, M. 1970. Bukimi no tani (the uncanny valley). <i>Energy 7, </i>4, 33–35.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">M<span class="s18">ORNINGSTAR</span>, C. <span class="s25">AND </span>F<span class="s18">ARMER</span>, F. R. 1991. <i>The Lessons of Lucasfilm’s Habitat</i>. MIT Press, Cambridge, MA.</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">273–302.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">M<span class="s18">ORRIS</span>, D., K<span class="s18">ELLAND</span>, M., <span class="s25">AND </span>L<span class="s18">LOYD</span>, D. 2005. <i>Machinima: Making Animated Movies in 3D Virtual Environ- ments</i>. Muska &amp; Lipman/Premier-Trade, Cincinnati, OH.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">M<span class="s18">OSS</span>, W., Y<span class="s18">EH</span>, H., H<span class="s18">ONG</span>, J.-M., L<span class="s18">IN</span>, M. C., <span class="s25">AND </span>M<span class="s18">ANOCHA</span>, D. 2010. Sounding liquids: Automatic sound synthesis from fluid simulation. <i>ACM Trans. Graph. 29</i>, 21:1–21:13.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">N<span class="s18">AKAMOTO</span><a href="http://www.bitcoin.org/bitcoin.pdf" class="a" target="_blank">, S. 2009. Bitcoin: A peer-to-peer electronic cash system. </a>http://www.bitcoin.org/bitcoin.pdf. (Last accessed 6/11).</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">N<span class="s18">ATIONAL</span><span class="s25"> </span>A<span class="s18">CADEMY</span><span class="s25"> OF </span>E<span class="s18">NGINEERING</span>. 2008. <i>Grand Challenges for Engineering</i>. National Academy of Engineer- ing, Washington, DC.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">N<span class="s18">AUMANN</span>, A. B., W<span class="s18">ECHSUNG</span>, I., <span class="s25">AND </span>H<span class="s18">URTIENNE</span>, J. 2010. Multimodal interaction: A suitable strategy for including older users? <i>Interact. Comput. 22</i>, 465–474.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">N<span class="s18">EWMAN</span>, R., M<span class="s18">ATSUMOTO</span>, Y., R<span class="s18">OUGEAUX</span>, S., <span class="s25">AND </span>Z<span class="s18">ELINSKY</span>, A. 2000. Real-time stereo tracking for head pose and gaze estimation. In <i>Proceedings of the 4th IEEE International Conference on Automatic Face and Gesture Recognition (FG’00)</i>. IEEE Computer Society, Washington, DC, 122.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">OA<span class="s18">UTH</span><span class="s25"> </span>C<span class="s18">OMMUNITY</span><a href="http://oauth.net/" class="a" target="_blank">. 2011. OAuth home page. http://oauth.net.</a> (Last accessed 5/11).</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">O’B<span class="s18">RIEN</span>, J. F., S<span class="s18">HEN</span>, C., <span class="s25">AND </span>G<span class="s18">ATCHALIAN</span>, C. M. 2002. Synthesizing sounds from rigid-body simulations. In <i>Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA’02)</i>. ACM, New York, NY, 175–181.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">O<span class="s18">LANO</span>, M., A<span class="s18">KELEY</span>, K., H<span class="s18">ART</span>, J. C., H<span class="s18">EIDRICH</span>, W., M<span class="s18">C</span>C<span class="s18">OOL</span>, M., M<span class="s18">ITCHELL</span>, J. L., <span class="s25">AND </span>R<span class="s18">OST</span>, R. 2004. Real-time</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">shading. In <i>ACM SIGGRAPH 2004 Course Notes (SIGGRAPH’04)</i>. ACM, New York, NY.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">O<span class="s18">LANO</span>, M. <span class="s25">AND </span>L<span class="s18">ASTRA</span>, A. 1998. A shading language on graphics hardware: The pixelflow shading sys- tem. In <i>Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH’98)</i>. ACM, New York, NY, 159–168.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">O<span class="s18">PEN</span><span class="s25"> </span>C<span class="s18">OBALT</span><span class="s25"> </span>P<span class="s18">ROJECT</span><a href="http://www.opencobalt.org/" class="a" target="_blank">. 2011. Open Cobalt website. </a>http://www.opencobalt.org. (Last accessed 3/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">O<span class="s18">PEN</span><span class="s25"> </span>W<span class="s18">ONDERLAND</span><span class="s25"> </span>F<span class="s18">OUNDATION</span>. 2011. Open Wonderland website. http://www.openwonderland.org. (Last accessed 3/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">O<span class="s18">PEN</span>ID F<span class="s18">OUNDATION</span><a href="http://openid.net/" class="a" target="_blank">. 2011. OpenID Foundation homepage. </a><a href="http://openid.net/" target="_blank">http://openid.net.</a></p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">O<span class="s18">PEN</span>S<span class="s18">IMULATOR</span><span class="s25"> </span>P<span class="s18">ROJECT</span><a href="http://www.opensimulator.org/" class="a" target="_blank">. 2011. OpenSimulator website. </a>http://www.opensimulator.org. (Last accessed 3/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">O<span class="s18">TADUY</span>, M. A., I<span class="s18">GARASHI</span>, T., <span class="s25">AND </span>L<span class="s18">A</span>V<span class="s18">IOLA</span>, J<span class="s18">R</span>., J. J. 2009. Interaction: Interfaces, algorithms, and applications.</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">In <i>ACM SIGGRAPH 2009 Courses (SIGGRAPH’09)</i>. ACM, New York, NY, 14:1–14:66.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">P<span class="s18">ALEARI</span>, M. <span class="s25">AND </span>L<span class="s18">ISETTI</span>, C. L. 2006. Toward multimodal fusion of affective cues. In <i>Proceedings of the 1st ACM International Workshop on Human-Centered Multimedia (HCM’06)</i>. ACM, New York, NY, 99–108.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-top: 5pt;padding-left: 21pt;text-indent: -16pt;text-align: justify;">P<span class="s18">IOTROWSKI</span>, J. <span class="s25">AND </span>T<span class="s18">ERZIS</span><a href="http://www.economist.com/blogs/" class="a" target="_blank">, G. 2011. Virtual currency: Bits and bob. </a>http://www.economist.com/blogs/ babbage/2011/06/virtual-currency. (Last accessed 6/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">P<span class="s18">OCKET</span><span class="s25"> </span>M<span class="s18">ETAVERSE</span><a href="http://www.pocketmetaverse.com/" class="a" target="_blank">. 2009. Pocket Metaverse website. http://www.pocketmetaverse.com.</a> (Last accessed 5/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -16pt;text-align: justify;">R<span class="s18">AHMAN</span>, A. S. M. M., H<span class="s18">OSSAIN</span>, S. K. A., <span class="s25">AND </span>S<span class="s18">ADDIK</span>, A. E. 2010. Bridging the gap between virtual and real world by bringing an interpersonal haptic communication system in Second Life. In <i>Proceedings of the IEEE International Symposium on Multimedia (ISM’10)</i>. IEEE Computer Society, Washington, DC, 228–235.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">R<span class="s18">ANKINE</span>, R., A<span class="s18">YROMLOU</span>, M., A<span class="s18">LIZADEH</span>, H., <span class="s25">AND </span>R<span class="s18">IVERS</span><a href="http://www/" class="a" target="_blank">, W. 2011. Kinect Second Life interface. </a>http://www. youtube.com/watch?v=KvVWjah_fXU. (Last accessed 5/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">R<span class="s18">AYMOND</span>, E. S. 2001. <i>The Cathedral and the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary</i>. O’Reilly &amp; Associates, Inc., Sebastopol, CA.</p><p class="s2" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s25">REAL</span><span class="s27">X</span><span class="s25">TEND </span>A<span class="s18">SSOCIATION</span><a href="http://realxtend.wordpress.com/" class="a" target="_blank">. 2011. realXtend website. </a>http://realxtend.wordpress.com. (Last accessed 12/11). R<span class="s18">ECANZONE</span>, G. H. 2003. Auditory influences on visual temporal rate perception. <i>J. Neurophysiol. 89, </i>2, 1078–</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">1093.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">R<span class="s18">ED</span>D<span class="s18">WARF</span><span class="s25"> </span>S<span class="s18">ERVER</span><span class="s25"> </span>P<span class="s18">ROJECT</span><a href="http://www.reddwarfserver.org/" class="a" target="_blank">. 2011. RedDwarf server website. http://www.reddwarfserver.org.</a> (Last accessed 6/11).</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">R<span class="s18">EED</span>, D. P. 2005. Designing croquet’s teatime: A real-time, temporal environment for active object cooper- ation. In <i>Companion to the 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA’05)</i>. ACM, New York, NY, 7.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -16pt;text-align: justify;">R<span class="s18">OH</span>, C. H. <span class="s25">AND </span>L<span class="s18">EE</span>, W. B. 2005. Image blending for virtual environment construction based on tip model. In <i>Proceedings of the 4th WSEAS International Conference on Signal Processing, Robotics and Automation</i>. World Scientific and Engineering Academy and Society (WSEAS), Stevens Point, WI. 26:1–26:5.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">R<span class="s18">OST</span>, R. J., L<span class="s18">ICEA</span>-K<span class="s18">ANE</span>, B., G<span class="s18">INSBURG</span>, D., K<span class="s18">ESSENICH</span>, J. M., L<span class="s18">ICHTENBELT</span>, B., M<span class="s18">ALAN</span>, H., <span class="s25">AND </span>W<span class="s18">EIBLEN</span>, M. 2009.</p><p class="s3" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">OpenGL Shading Language <span class="s2">3rd Ed. Addison-Wesley Professional, Boston, MA.</span></p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">S<span class="s18">ALLNA</span>¨<span class="s18">S</span>, E.-L. 2010. Haptic feedback increases perceived social presence. In <i>Proceedings of the International Conference on Haptics - Generating and Perceiving Tangible Sensations: Part II (EuroHaptics’10)</i>. Lecture Notes in Computer Science, vol. 6192, Springer-Verlag, Berlin, Heidelberg, 178–185.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">S<span class="s18">CHMIDT</span>, K. O. <span class="s25">AND </span>D<span class="s18">IONISIO</span>, J. D. N. 2008. Texture map based sound synthesis in rigid-body simulations. In</p><p class="s3" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">ACM SIGGRAPH 2008 Posters<span class="s2">. ACM, New York, NY, 11:1.</span></p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">S<span class="s18">ENG</span>, D. <span class="s25">AND </span>W<span class="s18">ANG</span>, H. 2009. Realistic real-time rendering of 3D terrain scenes based on OpenGL. In <i>Pro- ceedings of the 1st IEEE International Conference on Information Science and Engineering (ICISE’09)</i>. IEEE Computer Society, Washington, DC, 2121–2124.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">S<span class="s18">EYAMA</span>, J. <span class="s25">AND </span>N<span class="s18">AGAYAMA</span>, R. S. 2007. The uncanny valley: Effect of realism on the impression of artificial human faces. <i>Presence: Teleoper. Virtual Environ. 16</i>, 337–351.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">S<span class="s18">EYAMA</span>, J. <span class="s25">AND </span>N<span class="s18">AGAYAMA</span>, R. S. 2009. Probing the uncanny valley with the eye size aftereffect. <i>Presence: Teleoper. Virtual Environ. 18</i>, 321–339.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -16pt;text-align: justify;">S<span class="s18">HAH</span>, M. 2007. Image-space approach to real-time realistic rendering. Ph.D. dissertation, University of Central Florida, Orlando, FL. AAI3302925.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">S<span class="s18">HAH</span>, M. A., K<span class="s18">ONTTINEN</span>, J., <span class="s25">AND </span>P<span class="s18">ATTANAIK</span>, S. 2009. Image-space subsurface scattering for interactive render- ing of deformable translucent objects. <i>IEEE Comput. Graph. Appl. 29</i>, 66–78.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">S<span class="s18">HORT</span>, J., W<span class="s18">ILLIAMS</span>, E., <span class="s25">AND </span>C<span class="s18">HRISTIE</span>, B. 1976. <i>The Social Psychology of Telecommunications</i>. Wiley, London,</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">U.K.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">S<span class="s18">MART</span>, J., C<span class="s18">ASCIO</span>, J., <span class="s25">AND </span>P<span class="s18">ATTENDORF</span>, J. 2007. Metaverse roadmap overview. http://www.metaverseroadmap. org. (Last accessed 2/11).</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -16pt;text-align: justify;">S<span class="s18">MURRAYINCHESTER</span><span class="s25"> </span>(AV) <span class="s25">AND  </span>V<span class="s18">OIDVECTOR</span><span class="s25"> </span><a href="http://commons.wikimedia/" class="a" target="_blank">(AV). 2008. Mori uncanny valley.svg. </a>http://commons.wikimedia. org/wiki/File:Mori_Uncanny_Valley.svg under the Creative Commons Attribution-Share Alike 3.0 license. (Last accessed 6/11).</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -16pt;text-align: left;">S<span class="s18">TOLL</span>, C., G<span class="s18">ALL</span>, J., <span class="s25">DE </span>A<span class="s18">GUIAR</span>, E., T<span class="s18">HRUN</span>, S., <span class="s25">AND </span>T<span class="s18">HEOBALT</span>, C. 2010. Video-based reconstruction of animatable human characters. <i>ACM Trans. Graph. 29</i>, 139:1–139:10.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: left;">S<span class="s18">UBRAMANIAN</span>, S., G<span class="s18">UTWIN</span>, C., N<span class="s18">ACENTA</span>, M., P<span class="s18">OWER</span>, C., <span class="s25">AND </span>J<span class="s18">UN</span>, L. 2005. Haptic and tactile feedback in directed movements. In <i>Proceedings of the Conference on Guidelines on Tactile and Haptic Interactions</i>.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">S<span class="s18">UN</span>, W. <span class="s25">AND </span>M<span class="s18">UKHERJEE</span>, A. 2006. Generalized wavelet product integral for rendering dynamic glossy objects.</p><p class="s3" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">ACM Trans. Graph. 25<span class="s2">, 955–966.</span></p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">S<span class="s18">UNG</span>, J. <span class="s25">AND </span>K<span class="s18">IM</span>, D. 2009. Real-time facial expression recognition using STAAM and layered GDA classifier.</p><p class="s3" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Image Vision Comput. 27<span class="s2">, 1313–1325.</span></p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">T<span class="s18">AYLOR</span>, M. T., C<span class="s18">HANDAK</span>, A., A<span class="s18">NTANI</span>, L., <span class="s25">AND </span>M<span class="s18">ANOCHA</span>, D. 2009. RESound: Interactive sound rendering for dynamic virtual environments. In <i>Proceedings of the 17th ACM International Conference on Multimedia (MM’09)</i>. ACM, New York, NY, 271–280.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-top: 5pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">T<span class="s18">INWELL</span>, A. <span class="s25">AND </span>G<span class="s18">RIMSHAW</span>, M. 2009. Bridging the uncanny: An impossible traverse? In <i>Proceedings of the 13th International MindTrek Conference: Everyday Life in the Ubiquitous Era (MindTrek’09)</i>. ACM, New York, NY, 66–73.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">T<span class="s18">INWELL</span>, A., G<span class="s18">RIMSHAW</span>, M., N<span class="s18">ABI</span>, D. A., <span class="s25">AND </span>W<span class="s18">ILLIAMS</span>, A. 2011. Facial expression of emotion and perception of the uncanny valley in virtual characters. <i>Comput. Hum. Behav. 27</i>, 741–749.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">T<span class="s18">RATTNER</span>, C., S<span class="s18">TEURER</span>, M. E., <span class="s25">AND </span>K<span class="s18">APPE</span>, F. 2010. Socializing virtual worlds with facebook: A prototypical implementation of an expansion pack to communicate between facebook and opensimulator based virtual worlds. In <i>Proceedings of the 14th International Academic MindTrek Conference: Envisioning Future Media Environments (MindTrek’10)</i>. ACM, New York, NY, 158–160.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">T<span class="s18">RIESCH</span>, J. <span class="s25">AND VON DER </span>M<span class="s18">ALSBURG</span>, C. 1998. Robotic gesture recognition by cue combination. In <i>Informatik ’98: Informatik zwischen Bild und Sprache</i>, J. Dassow and R. Kruse, Eds., Springer-Verlag, Magdeburg, Germany, 223–232.</p><p class="s2" style="padding-top: 2pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">T<span class="s18">SETSERUKOU</span>, D. 2010. HaptiHug: A novel haptic display for communication of hug over a distance. In <i>Pro- ceedings of the International Conference on Haptics: Generating and perceiving tangible sensations, Part I (EuroHaptics’10)</i>. Lecture Notes in Computer Science, vol. 6191, Springer-Verlag, Berlin, Heidelberg, 340–347.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="3" height="1" alt="image" src="2480741.2480751_files/Image_018.png"/></span></p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">T<span class="s18">SETSERUKOU</span>, D., N<span class="s18">EVIAROUSKAYA</span>, A., P<span class="s18">RENDINGER</span>, H., I<span class="s18">SHIZUKA</span>, M., <span class="s25">AND </span>T<span class="s18">ACHI</span>, S. 2010. iFeel IM: Innovative</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;text-align: justify;">real-time communication system with rich emotional and haptic channels. In <i>Proceedings of the 28th of the International Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA’10)</i>. ACM, New York, NY, 3031–3036.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">T<span class="s18">URKLE</span>, S. 1995. <i>Life on the Screen: Identity in the Age of the Internet</i>. Simon &amp; Schuster Trade, New York, NY.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">T<span class="s18">URKLE</span>, S. 2007. <i>Evocative Objects: Things We Think With</i>. MIT Press, Cambridge, MA.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">T<span class="s18">WITTER</span><a href="http://dev.twitter.com/doc" class="a" target="_blank">. 2011. Twitter API documentation website. </a>http://dev.twitter.com/doc. (Last accessed 5/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">U<span class="s18">EDA</span>, Y., H<span class="s18">IROTA</span>, M., <span class="s25">AND </span>S<span class="s18">AKATA</span>, T. 2006. Vowel synthesis based on the spectral morphing and its application to speaker conversion. In <i>Proceedings of the 1st International Conference on Innovative Computing, Information and Control - Volume 2 (ICICIC’06)</i>. IEEE Computer Society, Washington, DC, 738–741.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;"><span class="s25">VAN DEN </span>D<span class="s18">OEL</span>, K., K<span class="s18">RY</span>, P. G., <span class="s25">AND </span>P<span class="s18">AI</span>, D. K. 2001. FoleyAutomatic: Physically-based sound effects for interactive simulation and animation. In <i>Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH’01)</i>. ACM, New York, NY, 537–544.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">V<span class="s18">IGNAIS</span>, N., K<span class="s18">ULPA</span>, R., C<span class="s18">RAIG</span>, C., B<span class="s18">RAULT</span>, S., M<span class="s18">ULTON</span>, F., <span class="s25">AND </span>B<span class="s18">IDEAU</span>, B. 2010. Influence of the graphical levels</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;text-align: justify;">of detail of a virtual thrower on the perception of the movement. <i>Presence: Teleoper. Virtual Environ. 19</i>, 243–252.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">W<span class="s18">ACHS</span>, J. P., K<span class="s18">O</span>¨ <span class="s18">LSCH</span>, M., S<span class="s18">TERN</span>, H., <span class="s18">AND  </span>E<span class="s18">DAN</span>, Y. 2011. Vision-based hand-gesture applications. <i>Commun.</i></p><p class="s3" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">ACM 54<span class="s2">, 60–71.</span></p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">W<span class="s18">ADLEY</span>, G., G<span class="s18">IBBS</span>, M. R., <span class="s25">AND </span>D<span class="s18">UCHENEAUT</span>, N. 2009. You can be too rich: Mediated communication in a virtual world. In <i>Proceedings of the 21st Annual Conference of the Australian Computer-Human Interaction Special Interest Group: Design: Open 24/7 (OZCHI’09)</i>. ACM, New York, NY, 49–56.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">W<span class="s18">ALDO</span>, J. 2008. Scaling in games and virtual worlds. <i>Commun. ACM 51</i>, 38–44.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">W<span class="s18">ANG</span>, L., L<span class="s18">IN</span>, Z., F<span class="s18">ANG</span>, T., Y<span class="s18">ANG</span>, X., Y<span class="s18">U</span>, X., <span class="s25">AND </span>K<span class="s18">ANG</span>, S. B. 2006. Real-time rendering of realistic rain. In</p><p class="s3" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">ACM SIGGRAPH 2006 Sketches (SIGGRAPH’06)<span class="s2">. ACM, New York, NY.</span></p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 119%;text-align: left;">W<span class="s18">EISER</span>, M. 1991. The computer for the 21st century. <i>Sci. Am. 265</i>, 94–102. W<span class="s18">EISER</span>, M. 1993. Ubiquitous computing. <i>Computer 26</i>, 71–72.</p><p class="s2" style="padding-left: 21pt;text-indent: -15pt;text-align: justify;">W<span class="s18">ILKES</span><a href="http://www.infoq.com/" class="a" target="_blank">, I. 2008. Second life: How it works (and how it doesn’t). QCon 2008. </a>http://www.infoq.com/ presentations/Second-Life-Ian-Wilkes. (Last accessed 6/11).</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">W<span class="s18">ITHANA</span>, A., K<span class="s18">ONDO</span>, M., M<span class="s18">AKINO</span>, Y., K<span class="s18">AKEHI</span>, G., S<span class="s18">UGIMOTO</span>, M., <span class="s25">AND </span>I<span class="s18">NAMI</span>, M. 2010. ImpAct: Immersive haptic</p><p class="s2" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">stylus to enable direct touch and manipulation for surface computing. <i>Comput. Entertain. 8</i>, 9:1–9:16.</p><p class="s2" style="padding-top: 1pt;padding-left: 21pt;text-indent: -15pt;text-align: justify;">X<span class="s18">U</span>, N., S<span class="s18">HAO</span>, X., <span class="s25">AND </span>Y<span class="s18">ANG</span>, Z. 2008. A novel voice morphing system using bi-gmm for high quality trans- formation. In <i>Proceedings of the 9th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing</i>. IEEE Computer Society, Washington, DC, 485–489.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 119%;text-align: justify;">Y<span class="s18">AHOO</span>! I<span class="s18">NC</span><a href="http://www.flickr.com/services/api" class="a" target="_blank">. 2011. Flickr Services website. http://www.flickr.com/services/api.</a> (Last accessed 5/11). Z<span class="s18">HAO</span>, Q. 2011. 10 scientific problems in virtual reality. <i>Commun. ACM 54</i>, 116–118.</p><p class="s2" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">Z<span class="s18">HENG</span>, C. <span class="s25">AND </span>J<span class="s18">AMES</span>, D. L. 2009. Harmonic fluids. <i>ACM Trans. Graph. 28</i>, 37:1–37:12.</p><p class="s2" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">Z<span class="s18">HENG</span>, C. <span class="s25">AND </span>J<span class="s18">AMES</span>, D. L. 2010. Rigid-body fracture sound with precomputed soundbanks. <i>ACM Trans.</i></p><p class="s3" style="padding-left: 21pt;text-indent: 0pt;line-height: 9pt;text-align: justify;">Graph. 29<span class="s2">, 69:1–69:13.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">Received July 2011; revised February 2012; accepted March 2012</p></body></html>
